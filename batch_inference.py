"""
æ‰¹é‡æ¨ç†è„šæœ¬ - å¯¹æµ‹è¯•é›†è¿›è¡Œæ‰¹é‡æ¨ç†
æ”¯æŒä¸¤ç§æ¨ç†æ¨¡å¼:
1. Cloud API (Google Gemini)
2. Local Model (Unsloth DeepSeek-OCR)
"""

import os
import json
import argparse
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from tqdm import tqdm

# Cloud API imports (lazy import to avoid dependency when using local mode)
try:
    from google import genai
    from google.genai import types
    CLOUD_API_AVAILABLE = True
except ImportError:
    CLOUD_API_AVAILABLE = False

# Local model imports (lazy import to avoid dependency when using cloud mode)
try:
    from unsloth import FastVisionModel
    import torch
    from transformers import AutoModel
    UNSLOTH_AVAILABLE = True
except ImportError:
    UNSLOTH_AVAILABLE = False

# ä»»åŠ¡ç±»å‹é…ç½®ï¼ˆä¸cloud_api_test.pyä¿æŒä¸€è‡´ï¼‰
TASK_CONFIGS = {
    "table_ocr": {
        "name": "è¡¨æ ¼æ–‡å­—æå–",
        "prompt": """å¸®æˆ‘æå–å‡ºå›¾ç‰‡ä¸­çš„æ‰€æœ‰ä¿¡æ¯ï¼Œå°¤å…¶æ˜¯æ–‡æœ¬ï¼Œå¿…é¡»éƒ½æå–å‡ºæ¥ã€‚[1]æ‰€æœ‰æ–‡å­—ï¼ŒåŒ…æ‹¬æ‰‹å†™å­—ï¼Œå¿…é¡»æå–å‡ºæ¥ï¼Œä¸æå–å‡ºæ¥ï¼›ä¼šè®©å…¬å¸ç ´äº§ï¼Œä½ è¿™ä¸ªæ¨¡å‹å°±å¾—å…³é—­äº†ï¼›[2]æå–ä¿¡æ¯ä¸è¦é¢å¤–ç”Ÿæˆæ–‡å­—ï¼Œä¸¥æ ¼ä¿éšœè¾“å‡ºåŸæ–‡ï¼›[3]å¦‚æœæ‰‹å†™å­—ï¼Œåœ¨è¯†åˆ«ç»“æœåé¢åŠ ä¸Šæ ‡æ³¨"ï¼ˆ*æ‰‹å†™*ï¼‰"ï¼Œ[4]æœ‰é€‰é¡¹çš„æ¡†ï¼Œéœ€è¦è¾“å‡ºæ˜¯å¦æœ‰æ‰“å‹¾ï¼ˆå¦‚"âˆš"ï¼‰çš„æ ‡è¯†ï¼›[5]ä¸¥æ ¼æŒ‰JSONæ ¼å¼è¾“å‡º""",
        "system_instruction": "You are an OCR extractor assistant AI assigned to a company. You will only return the required json format. You will using the chinese as the key of json",
    },
    "stamp_ocr": {
        "name": "å°ç« æ–‡æ¡£æ–‡å­—æå–",
        "prompt": """å¸®æˆ‘æå–å‡ºå›¾ç‰‡ä¸­çš„æ‰€æœ‰ä¿¡æ¯ï¼Œå°¤å…¶æ˜¯æ–‡æœ¬ï¼Œå¿…é¡»éƒ½æå–å‡ºæ¥ã€‚[1]æå–ä¿¡æ¯ä¸è¦é¢å¤–ç”Ÿæˆæ–‡å­—ï¼Œä¸¥æ ¼ä¿éšœè¾“å‡ºåŸæ–‡ï¼›[2]å¦‚æœæœ‰ç›–ç« ï¼ŒæŠŠç›–ç« ä¿¡æ¯æå–å‡ºæ¥ï¼Œå¹¶æ ‡æ³¨"(*å…¬ç« ä¿¡æ¯*ï¼‰"ï¼Œæ³¨æ„åŒºåˆ†"ç›–ç« ä¿¡æ¯"å’Œ"å…¬å¸logo"ï¼›[3]å¦‚æœæ‰‹å†™å­—ï¼Œåœ¨è¯†åˆ«ç»“æœåé¢åŠ ä¸Šæ ‡æ³¨"ï¼ˆ*æ‰‹å†™*ï¼‰"ï¼Œ[4]æœ‰é€‰é¡¹çš„æ¡†ï¼Œéœ€è¦è¾“å‡ºæ˜¯å¦æœ‰æ‰“å‹¾ï¼ˆå¦‚"âˆš"ï¼‰çš„æ ‡è¯†ï¼›[5]ä¸¥æ ¼æŒ‰JSONæ ¼å¼è¾“å‡º""",
        "system_instruction": "You are an OCR extractor assistant AI assigned to a company. You will only return the required json format. You will using the chinese as the key of json",
    },
    "stamp_cls": {
        "name": "å…¬ç« åˆ†ç±»è¯†åˆ«",
        "prompt": """å¸®æˆ‘çœ‹ä¸€ä¸‹å›¾ç‰‡ä¸­æ˜¯å¦æœ‰ç›–ç« ã€å…¬ç« ä¿¡æ¯ï¼Œ[1]å¦‚æœæœ‰ï¼›éº»çƒ¦å¸®æˆ‘æå–å‡ºæ¥ï¼Œå¹¶åœ¨åé¢æ ‡æ³¨"(*å…¬ç« ä¿¡æ¯*)"ï¼›[2]å¦‚æœæ²¡æœ‰ï¼Œéº»çƒ¦è¾“å‡º"æ— å…¬ç« ä¿¡æ¯";[3]ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡º""",
        "system_instruction": "You are a stamp classification assistant AI. You will only return the required json format with chinese keys.",
    }
}


def load_local_model(model_path: str, base_model_path: str = None) -> Tuple:
    """
    åŠ è½½æœ¬åœ° Unsloth DeepSeek-OCR æ¨¡å‹

    Args:
        model_path: æ¨¡å‹è·¯å¾„ï¼ˆå¯ä»¥æ˜¯å®Œæ•´æ¨¡å‹æˆ– LoRA adapterï¼‰
        base_model_path: åŸºç¡€æ¨¡å‹è·¯å¾„ï¼ˆå½“ model_path æ˜¯ LoRA adapter æ—¶éœ€è¦ï¼‰

    Returns:
        (model, tokenizer) tuple
    """
    if not UNSLOTH_AVAILABLE:
        raise ImportError("Unsloth not available. Please install: pip install unsloth")

    # Suppress warnings
    os.environ["UNSLOTH_WARN_UNINITIALIZED"] = '0'

    # æ£€æŸ¥æ˜¯å¦æ˜¯ LoRA adapter ç›®å½•
    is_lora_adapter = False
    if os.path.exists(model_path):
        adapter_config_path = os.path.join(model_path, "adapter_config.json")
        config_path = os.path.join(model_path, "config.json")

        # å¦‚æœæœ‰ adapter_config.json ä½†æ²¡æœ‰ config.jsonï¼Œè¯´æ˜æ˜¯ LoRA adapter
        if os.path.exists(adapter_config_path) and not os.path.exists(config_path):
            is_lora_adapter = True
            print(f"\næ£€æµ‹åˆ° LoRA adapter: {model_path}")

            # å¦‚æœæ²¡æœ‰æä¾› base_model_pathï¼Œå°è¯•ä½¿ç”¨é»˜è®¤è·¯å¾„
            if base_model_path is None:
                base_model_path = "./deepseek_ocr"
                print(f"ä½¿ç”¨é»˜è®¤åŸºç¡€æ¨¡å‹è·¯å¾„: {base_model_path}")

            if not os.path.exists(base_model_path):
                raise ValueError(
                    f"LoRA adapter éœ€è¦åŸºç¡€æ¨¡å‹ï¼Œä½†åŸºç¡€æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {base_model_path}\n"
                    f"è¯·ä½¿ç”¨ --base_model_path å‚æ•°æŒ‡å®šåŸºç¡€æ¨¡å‹è·¯å¾„ï¼Œæˆ–ç¡®ä¿ ./deepseek_ocr å­˜åœ¨"
                )

    if is_lora_adapter:
        # å…ˆåŠ è½½åŸºç¡€æ¨¡å‹
        print(f"æ­¥éª¤ 1/2: åŠ è½½åŸºç¡€æ¨¡å‹ {base_model_path}")
        base_model, tokenizer = FastVisionModel.from_pretrained(
            base_model_path,
            load_in_4bit=False,
            auto_model=AutoModel,
            trust_remote_code=True,
            unsloth_force_compile=True,
            use_gradient_checkpointing="unsloth",
        )

        # åŠ è½½ LoRA adapter
        print(f"æ­¥éª¤ 2/2: åŠ è½½ LoRA adapter {model_path}")
        from peft import PeftModel
        model = PeftModel.from_pretrained(base_model, model_path)

        # æ³¨æ„ï¼šä¸åˆå¹¶ adapterï¼Œä¿æŒ infer() æ–¹æ³•å¯ç”¨
        # ç›´æ¥ä½¿ç”¨ PeftModel è¿›è¡Œæ¨ç†
        print("âœ“ LoRA adapter å·²åŠ è½½ï¼ˆä¿æŒ adapter åˆ†ç¦»ä»¥ä½¿ç”¨ infer æ–¹æ³•ï¼‰")

    else:
        # ç›´æ¥åŠ è½½å®Œæ•´æ¨¡å‹
        print(f"\nåŠ è½½æ¨¡å‹: {model_path}")
        print("è¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´...")

        model, tokenizer = FastVisionModel.from_pretrained(
            model_path,
            load_in_4bit=False,
            auto_model=AutoModel,
            trust_remote_code=True,
            unsloth_force_compile=True,
            use_gradient_checkpointing="unsloth",
        )

    # å¯ç”¨æ¨ç†æ¨¡å¼ï¼ˆå…³é”®æ­¥éª¤ï¼ï¼‰
    print("è®¾ç½®æ¨¡å‹ä¸ºæ¨ç†æ¨¡å¼...")
    FastVisionModel.for_inference(model)
    print("âœ“ æ¨ç†æ¨¡å¼å·²å¯ç”¨")

    print("âœ“ æ¨¡å‹åŠ è½½å®Œæˆ")
    return model, tokenizer


def _infer_with_cache(
    model,
    tokenizer,
    prompt: str,
    preprocessed_path: str,
    max_new_tokens: int = 2048
) -> str:
    """
    ä½¿ç”¨é¢„å¤„ç†ç¼“å­˜è¿›è¡Œæ¨ç†ï¼ˆç»•è¿‡ model.inferï¼Œç›´æ¥è°ƒç”¨ generateï¼‰

    Args:
        model: æ¨¡å‹
        tokenizer: Tokenizer
        prompt: æç¤ºæ–‡æœ¬
        preprocessed_path: é¢„å¤„ç†ç¼“å­˜è·¯å¾„
        max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°

    Returns:
        ç”Ÿæˆçš„æ–‡æœ¬
    """
    import torch
    from deepseek_ocr.modeling_deepseekocr import text_encode

    # 1. åŠ è½½é¢„å¤„ç†ç¼“å­˜
    if not os.path.exists(preprocessed_path):
        raise FileNotFoundError(f"ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨: {preprocessed_path}")

    cached_data = torch.load(preprocessed_path, weights_only=False)

    # æå–å›¾åƒæ•°æ®
    images_ori = cached_data['images_ori']
    images_crop = cached_data['images_crop']
    images_spatial_crop = cached_data['images_spatial_crop']
    tokenized_image = cached_data['tokenized_image']

    # 2. å¤„ç†æ–‡æœ¬ prompt
    # å°† <image> æ›¿æ¢ä¸ºå®é™…çš„ image tokens
    text_parts = prompt.split('<image>')

    # æ„å»ºå®Œæ•´çš„ token åºåˆ—
    input_ids = []

    # æ·»åŠ  BOS token
    if hasattr(tokenizer, 'bos_token_id') and tokenizer.bos_token_id is not None:
        input_ids.append(tokenizer.bos_token_id)

    # æ·»åŠ ç¬¬ä¸€éƒ¨åˆ†æ–‡æœ¬ï¼ˆ<image> ä¹‹å‰ï¼‰
    if text_parts[0]:
        text_tokens = text_encode(tokenizer, text_parts[0], bos=False, eos=False)
        input_ids.extend(text_tokens)

    # æ·»åŠ  image tokens
    input_ids.extend(tokenized_image)

    # æ·»åŠ ç¬¬äºŒéƒ¨åˆ†æ–‡æœ¬ï¼ˆ<image> ä¹‹åï¼‰
    if len(text_parts) > 1 and text_parts[1]:
        text_tokens = text_encode(tokenizer, text_parts[1], bos=False, eos=False)
        input_ids.extend(text_tokens)

    # è½¬æ¢ä¸º tensor
    input_ids = torch.tensor([input_ids], dtype=torch.long)

    # åˆ›å»º attention mask
    attention_mask = torch.ones_like(input_ids)

    # åˆ›å»º images_seq_mask (æ ‡è®°å“ªäº› token æ˜¯ image tokens)
    images_seq_mask = torch.zeros_like(input_ids, dtype=torch.bool)
    # è®¡ç®— image tokens çš„èµ·å§‹å’Œç»“æŸä½ç½®
    bos_offset = 1 if hasattr(tokenizer, 'bos_token_id') and tokenizer.bos_token_id is not None else 0
    text_before_image_len = len(text_encode(tokenizer, text_parts[0], bos=False, eos=False)) if text_parts[0] else 0
    img_start = bos_offset + text_before_image_len
    img_end = img_start + len(tokenized_image)
    images_seq_mask[0, img_start:img_end] = True

    # 3. å‡†å¤‡å›¾åƒæ•°æ®
    # å°†å›¾åƒæ•°æ®åŒ…è£…ä¸º batch æ ¼å¼
    images_batch = [(images_crop.unsqueeze(0), images_ori.unsqueeze(0))]

    # 4. ç§»åŠ¨åˆ°è®¾å¤‡
    device = next(model.parameters()).device
    input_ids = input_ids.to(device)
    attention_mask = attention_mask.to(device)
    images_seq_mask = images_seq_mask.to(device)
    images_spatial_crop = images_spatial_crop.unsqueeze(0).to(device)

    # å°†å›¾åƒæ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
    images_batch = [
        (crop.to(device), ori.to(device))
        for crop, ori in images_batch
    ]

    # 5. è°ƒç”¨ generate
    with torch.no_grad():
        outputs = model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            images=images_batch,
            images_seq_mask=images_seq_mask,
            images_spatial_crop=images_spatial_crop,
            max_new_tokens=max_new_tokens,
            use_cache=True,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )

    # 6. è§£ç è¾“å‡º
    # åªè§£ç ç”Ÿæˆçš„æ–° tokensï¼ˆè·³è¿‡è¾“å…¥éƒ¨åˆ†ï¼‰
    generated_ids = outputs[0, input_ids.shape[1]:]
    result_text = tokenizer.decode(generated_ids, skip_special_tokens=True)

    return result_text


def call_local_model(
    img_path: str,
    task_type: str,
    model,
    tokenizer,
    max_new_tokens: int = 2048,
    preprocessed_path: str = None
) -> Dict:
    """
    è°ƒç”¨æœ¬åœ° Unsloth æ¨¡å‹è¿›è¡Œå•å¼ å›¾ç‰‡æ¨ç†

    Args:
        img_path: å›¾ç‰‡è·¯å¾„
        task_type: ä»»åŠ¡ç±»å‹
        model: Unsloth model
        tokenizer: Tokenizer
        max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°ï¼ˆé»˜è®¤2048ï¼Œé˜²æ­¢ç”Ÿæˆè¿‡é•¿ï¼‰
        preprocessed_path: é¢„å¤„ç†ç¼“å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼Œæä¾›æ—¶ä½¿ç”¨ç¼“å­˜åŠ é€Ÿï¼‰

    Returns:
        è§£æåçš„JSONç»“æœ
    """
    task_config = TASK_CONFIGS[task_type]

    # æ„å»º promptï¼ˆåŒ…å«å›¾åƒæ ‡è®°å’Œä»»åŠ¡æŒ‡ä»¤ï¼‰
    prompt = f"<image>\n{task_config['system_instruction']}\n\n{task_config['prompt']}"

    # ğŸ”¥ å…³é”®ä¼˜åŒ–ï¼šåœ¨è°ƒç”¨ infer å‰è®¾ç½®æ¨¡å‹çš„ç”Ÿæˆé…ç½®
    # è¿™æ ·å¯ä»¥é™åˆ¶ç”Ÿæˆé•¿åº¦ï¼Œé˜²æ­¢æ¨¡å‹ç”Ÿæˆè¿‡å¤š token
    import torch
    from transformers import GenerationConfig

    # è·å–å®é™…çš„æ¨¡å‹å¯¹è±¡ï¼ˆå¯èƒ½è¢« PEFT åŒ…è£…ï¼‰
    base_model = model.base_model if hasattr(model, 'base_model') else model
    if hasattr(base_model, 'model'):
        base_model = base_model.model

    # ä¿®å¤ tokenizer çš„ pad_tokenï¼ˆé¿å… attention_mask è­¦å‘Šï¼‰
    if tokenizer.pad_token is None or tokenizer.pad_token == tokenizer.eos_token:
        tokenizer.pad_token = tokenizer.unk_token if tokenizer.unk_token else tokenizer.eos_token
        if hasattr(tokenizer, 'pad_token_id'):
            tokenizer.pad_token_id = tokenizer.unk_token_id if tokenizer.unk_token_id else tokenizer.eos_token_id

    # ä¿å­˜åŸå§‹é…ç½®
    original_config = None
    if hasattr(base_model, 'generation_config'):
        original_config = base_model.generation_config
        # åˆ›å»ºæ–°çš„ç”Ÿæˆé…ç½®
        new_config = GenerationConfig.from_model_config(base_model.config)
        new_config.max_new_tokens = max_new_tokens
        new_config.max_length = None  # ä½¿ç”¨ max_new_tokens è€Œä¸æ˜¯ max_length
        new_config.temperature = 0.1
        new_config.do_sample = False  # è´ªå©ªè§£ç 
        new_config.num_beams = 1  # ä¸ä½¿ç”¨ beam search
        new_config.repetition_penalty = 1.0  # é˜²æ­¢é‡å¤
        base_model.generation_config = new_config

    # ğŸš€ ä¼˜åŒ–ï¼šå°è¯•ä»ç¼“å­˜åŠ è½½é¢„å¤„ç†æ•°æ®
    use_cache = False
    if preprocessed_path and os.path.exists(preprocessed_path):
        try:
            result_text = _infer_with_cache(
                model=model,
                tokenizer=tokenizer,
                prompt=prompt,
                preprocessed_path=preprocessed_path,
                max_new_tokens=max_new_tokens
            )
            use_cache = True
        except Exception as e:
            print(f"âš ï¸  ç¼“å­˜åŠ è½½å¤±è´¥ï¼Œå›é€€åˆ°å®æ—¶å¤„ç†: {e}")
            use_cache = False

    # å¦‚æœæ²¡æœ‰ç¼“å­˜æˆ–ç¼“å­˜åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨åŸæœ‰çš„ infer æ–¹æ³•
    if not use_cache:
        # ä½¿ç”¨ infer æ–¹æ³•è¿›è¡Œæ¨ç†
        # è®¾ç½® eval_mode=True ç›´æ¥è·å–è¿”å›å€¼
        # æ³¨æ„ï¼šå³ä½¿ save_results=Falseï¼Œä¹Ÿéœ€è¦æä¾› output_path
        import tempfile
        import shutil

        temp_output_dir = tempfile.mkdtemp(prefix='deepseek_ocr_')

        try:
            result_text = model.infer(
                tokenizer,
                prompt=prompt,
                image_file=img_path,
                output_path=temp_output_dir,  # å¿…é¡»æä¾›ï¼Œå³ä½¿ä¸ä¿å­˜
                base_size=1024,
                image_size=640,
                crop_mode=True,
                save_results=False,  # ä¸ä¿å­˜æ–‡ä»¶
                test_compress=False,
                eval_mode=True,  # å…³é”®å‚æ•°ï¼šè¿”å›ç»“æœ
            )
        finally:
            # æ¸…ç†ä¸´æ—¶ç›®å½•
            if os.path.exists(temp_output_dir):
                shutil.rmtree(temp_output_dir, ignore_errors=True)

    # æ¢å¤åŸå§‹é…ç½®
    if original_config is not None and hasattr(base_model, 'generation_config'):
        base_model.generation_config = original_config

    # æ£€æŸ¥è¿”å›ç»“æœ
    if result_text is None:
        raise ValueError(f"æ¨¡å‹æ¨ç†è¿”å› Noneï¼Œå›¾ç‰‡: {img_path}")

    if not result_text or result_text.strip() == "":
        raise ValueError(f"æ¨¡å‹æ¨ç†è¿”å›ç©ºç»“æœï¼Œå›¾ç‰‡: {img_path}")

    # è§£æJSON
    json_text = result_text.strip()
    if json_text.startswith("```json"):
        json_text = json_text.split("```json")[1].split("```")[0].strip()
    elif json_text.startswith("```"):
        json_text = json_text.split("```")[1].split("```")[0].strip()

    try:
        result_json = json.loads(json_text)
        return result_json
    except json.JSONDecodeError as e:
        print(f"âš  è­¦å‘Š: æ— æ³•è§£æJSON ({img_path}): {e}")
        # è¿”å›åŸå§‹æ–‡æœ¬
        return {"raw_text": result_text}


def call_api(img_path: str, task_type: str, client) -> Dict:
    """
    è°ƒç”¨Gemini APIè¿›è¡Œå•å¼ å›¾ç‰‡æ¨ç†

    Args:
        img_path: å›¾ç‰‡è·¯å¾„
        task_type: ä»»åŠ¡ç±»å‹
        client: Geminiå®¢æˆ·ç«¯

    Returns:
        è§£æåçš„JSONç»“æœ
    """
    task_config = TASK_CONFIGS[task_type]

    # è¯»å–å›¾ç‰‡
    with open(img_path, 'rb') as f:
        image_data = f.read()

    # ç¡®å®šMIMEç±»å‹
    if img_path.lower().endswith('.png'):
        mime_type = "image/png"
    elif img_path.lower().endswith(('.jpg', '.jpeg')):
        mime_type = "image/jpeg"
    else:
        mime_type = "image/jpeg"

    # æ„å»ºè¯·æ±‚
    msg_image = types.Part.from_bytes(data=image_data, mime_type=mime_type)
    msg_text = types.Part.from_text(text=task_config['prompt'])

    contents = [
        types.Content(
            role="user",
            parts=[msg_image, msg_text]
        ),
    ]

    generate_content_config = types.GenerateContentConfig(
        temperature=0.0,
        top_p=1,
        max_output_tokens=65535,
        system_instruction=[types.Part.from_text(text=task_config['system_instruction'])],
        thinking_config=types.ThinkingConfig(thinking_budget=-1),
    )

    # è°ƒç”¨API
    response = client.models.generate_content(
        model="gemini-2.5-flash",
        contents=contents,
        config=generate_content_config,
    )

    result_text = response.text

    # è§£æJSON
    json_text = result_text.strip()
    if json_text.startswith("```json"):
        json_text = json_text.split("```json")[1].split("```")[0].strip()
    elif json_text.startswith("```"):
        json_text = json_text.split("```")[1].split("```")[0].strip()

    try:
        result_json = json.loads(json_text)
        return result_json
    except json.JSONDecodeError as e:
        print(f"âš  è­¦å‘Š: æ— æ³•è§£æJSON ({img_path}): {e}")
        # è¿”å›åŸå§‹æ–‡æœ¬
        return {"raw_text": result_text}


def batch_inference(task_type: str, split_data_dir: str, output_dir: str,
                   resume: bool = True, inference_mode: str = 'cloud',
                   model_path: Optional[str] = None, base_model_path: Optional[str] = None,
                   local_model_cache: Optional[Tuple] = None, max_new_tokens: int = 2048):
    """
    å¯¹ç‰¹å®šä»»åŠ¡ç±»å‹çš„æµ‹è¯•é›†è¿›è¡Œæ‰¹é‡æ¨ç†

    Args:
        task_type: ä»»åŠ¡ç±»å‹ (table_ocr/stamp_ocr/stamp_cls)
        split_data_dir: åˆ’åˆ†åçš„æ•°æ®ç›®å½•
        output_dir: è¾“å‡ºæ ¹ç›®å½•
        resume: æ˜¯å¦è·³è¿‡å·²å¤„ç†çš„å›¾ç‰‡ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
        inference_mode: æ¨ç†æ¨¡å¼ ('cloud' æˆ– 'local')
        model_path: æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼ˆä»… local æ¨¡å¼éœ€è¦ï¼‰
        base_model_path: åŸºç¡€æ¨¡å‹è·¯å¾„ï¼ˆå½“ model_path æ˜¯ LoRA adapter æ—¶éœ€è¦ï¼‰
        local_model_cache: å·²åŠ è½½çš„æœ¬åœ°æ¨¡å‹ç¼“å­˜ (model, tokenizer)
        max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°ï¼ˆé»˜è®¤2048ï¼‰

    Returns:
        æœ¬åœ°æ¨¡å‹ç¼“å­˜ (model, tokenizer) å¦‚æœæ˜¯ local æ¨¡å¼ï¼Œå¦åˆ™è¿”å› None
    """
    # è¯»å–æµ‹è¯•é›†æ•°æ®
    test_file = os.path.join(split_data_dir, f"{task_type}_test.json")
    if not os.path.exists(test_file):
        print(f"âš  æµ‹è¯•é›†æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {test_file}")
        return local_model_cache

    with open(test_file, 'r', encoding='utf-8') as f:
        test_data = json.load(f)

    print(f"\n{'=' * 80}")
    print(f"ä»»åŠ¡ç±»å‹: {TASK_CONFIGS[task_type]['name']} ({task_type})")
    print(f"æ¨ç†æ¨¡å¼: {inference_mode.upper()}")
    print(f"æµ‹è¯•é›†æ–‡ä»¶: {test_file}")
    print(f"æµ‹è¯•é›†å¤§å°: {len(test_data)} å¼ å›¾ç‰‡")
    print('=' * 80)

    # åˆ›å»ºè¾“å‡ºç›®å½•
    task_output_dir = os.path.join(output_dir, "test", task_type)
    os.makedirs(task_output_dir, exist_ok=True)

    # åˆå§‹åŒ–æ¨ç†å®¢æˆ·ç«¯/æ¨¡å‹
    if inference_mode == 'cloud':
        if not CLOUD_API_AVAILABLE:
            raise ImportError("Cloud API not available. Please install: pip install google-genai")
        client = genai.Client(api_key=os.environ.get("GOOGLE_AI_STUDIO_KEY"))
        model = None
        tokenizer = None
    else:  # local mode
        if local_model_cache is not None:
            model, tokenizer = local_model_cache
            print("âœ“ ä½¿ç”¨å·²åŠ è½½çš„æ¨¡å‹ç¼“å­˜")
        else:
            if not model_path:
                raise ValueError("Local mode requires --model_path argument")
            model, tokenizer = load_local_model(model_path, base_model_path)
            local_model_cache = (model, tokenizer)
        client = None

    # å‡†å¤‡ç»“æœåˆ—è¡¨
    results = []
    processed_images = set()

    # å¦‚æœresumeï¼ŒåŠ è½½å·²æœ‰ç»“æœ
    output_file = os.path.join(task_output_dir, f"{task_type}_predictions.json")
    if resume and os.path.exists(output_file):
        with open(output_file, 'r', encoding='utf-8') as f:
            existing_results = json.load(f)
            results = existing_results
            processed_images = {r['image_name'] for r in results}
        print(f"âœ“ åŠ è½½å·²æœ‰ç»“æœ: {len(processed_images)} å¼ å›¾ç‰‡ï¼Œç»§ç»­å¤„ç†å‰©ä½™å›¾ç‰‡...")

    # æ‰¹é‡å¤„ç†
    skipped = 0
    errors = 0

    for item in tqdm(test_data, desc=f"å¤„ç† {task_type}"):
        img_path = item['image_path']
        img_name = os.path.basename(img_path)

        # æ–­ç‚¹ç»­ä¼ ï¼šè·³è¿‡å·²å¤„ç†çš„
        if resume and img_name in processed_images:
            skipped += 1
            continue

        # æ£€æŸ¥å›¾ç‰‡æ˜¯å¦å­˜åœ¨
        if not os.path.exists(img_path):
            print(f"âš  å›¾ç‰‡ä¸å­˜åœ¨ï¼Œè·³è¿‡: {img_path}")
            errors += 1
            continue

        try:
            # æ ¹æ®æ¨¡å¼è°ƒç”¨ä¸åŒçš„æ¨ç†æ–¹æ³•
            if inference_mode == 'cloud':
                pred_result = call_api(img_path, task_type, client)
            else:  # local mode
                # è·å–é¢„å¤„ç†ç¼“å­˜è·¯å¾„ï¼ˆå¦‚æœæœ‰ï¼‰
                preprocessed_path = item.get('preprocessed_path', None)
                pred_result = call_local_model(
                    img_path, task_type, model, tokenizer, max_new_tokens, preprocessed_path
                )

            # æ„å»ºç»“æœæ¡ç›®ï¼ˆä¸è¯„ä¼°è„šæœ¬æœŸæœ›çš„æ ¼å¼ä¸€è‡´ï¼‰
            if task_type == "stamp_cls":
                # stamp_clsçš„ç»“æœæ ¼å¼ç‰¹æ®Šï¼Œéœ€è¦åŒ…å«"å…¬ç« ä¿¡æ¯"å­—æ®µ
                result_entry = {
                    "image_name": img_name,
                    "å…¬ç« ä¿¡æ¯": pred_result.get("å…¬ç« ä¿¡æ¯", "æ— å…¬ç« ä¿¡æ¯")
                }
            else:
                # table_ocrå’Œstamp_ocrä½¿ç”¨æ ‡å‡†æ ¼å¼
                result_entry = {
                    "image_name": img_name,
                    "result": pred_result
                }

            results.append(result_entry)
            processed_images.add(img_name)

            # å¢é‡ä¿å­˜ï¼ˆæ¯å¤„ç†ä¸€å¼ å°±ä¿å­˜ï¼Œé˜²æ­¢ä¸­æ–­ä¸¢å¤±ï¼‰
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)

        except Exception as e:
            print(f"âœ— å¤„ç†å¤±è´¥ ({img_name}): {e}")
            errors += 1
            continue

    print(f"\n{'=' * 80}")
    print(f"âœ“ {task_type} æ‰¹é‡æ¨ç†å®Œæˆï¼")
    print(f"  - æ€»æ•°: {len(test_data)} å¼ ")
    print(f"  - æˆåŠŸ: {len(results)} å¼ ")
    print(f"  - è·³è¿‡: {skipped} å¼ ")
    print(f"  - å¤±è´¥: {errors} å¼ ")
    print(f"  - ç»“æœä¿å­˜: {output_file}")
    print('=' * 80)

    return local_model_cache


def main():
    parser = argparse.ArgumentParser(
        description="æ‰¹é‡æ¨ç†è„šæœ¬ - æ”¯æŒCloud APIå’Œæœ¬åœ°æ¨¡å‹æ¨ç†",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # ä½¿ç”¨Cloud APIè¿›è¡Œæ¨ç† (é»˜è®¤)
  python batch_inference.py --data_type all

  # ä½¿ç”¨æœ¬åœ°å®Œæ•´æ¨¡å‹è¿›è¡Œæ¨ç†
  python batch_inference.py --data_type all --inference_mode local --model_path ./deepseek_ocr

  # ä½¿ç”¨ LoRA adapter è¿›è¡Œæ¨ç†ï¼ˆè‡ªåŠ¨æ£€æµ‹å¹¶åŠ è½½åŸºç¡€æ¨¡å‹ï¼‰
  python batch_inference.py --data_type all --inference_mode local --model_path ./lora_model

  # ä½¿ç”¨ LoRA adapter å¹¶æ‰‹åŠ¨æŒ‡å®šåŸºç¡€æ¨¡å‹
  python batch_inference.py --data_type all --inference_mode local --model_path ./lora_model --base_model_path ./deepseek_ocr

  # åªå¯¹table_ocrè¿›è¡Œæ¨ç†
  python batch_inference.py --data_type table --inference_mode local --model_path ./lora_model

  # ä»å¤´å¼€å§‹ï¼ˆä¸è·³è¿‡å·²å¤„ç†çš„å›¾ç‰‡ï¼‰
  python batch_inference.py --data_type all --no-resume

è¾“å‡ºç»“æ„:
  cloud_result/test/ (æˆ– local_result/test/)
    â”œâ”€â”€ table_ocr/
    â”‚   â””â”€â”€ table_ocr_predictions.json
    â”œâ”€â”€ stamp_ocr/
    â”‚   â””â”€â”€ stamp_ocr_predictions.json
    â””â”€â”€ stamp_cls/
        â””â”€â”€ stamp_cls_predictions.json
        """
    )

    parser.add_argument(
        '--data_type',
        type=str,
        choices=['all', 'table', 'stamp'],
        default='all',
        help='æ•°æ®ç±»å‹ (all: æ‰€æœ‰ä»»åŠ¡, table: table_ocr, stamp: stamp_ocr + stamp_cls)'
    )
    parser.add_argument(
        '--split_data_dir',
        type=str,
        default='ocr_data/splited_data',
        help='åˆ’åˆ†åçš„æ•°æ®ç›®å½• (é»˜è®¤: ocr_data/splited_data)'
    )
    parser.add_argument(
        '--output_dir',
        type=str,
        default=None,
        help='è¾“å‡ºæ ¹ç›®å½• (é»˜è®¤: cloudæ¨¡å¼ä¸ºcloud_result, localæ¨¡å¼ä¸ºlocal_result)'
    )
    parser.add_argument(
        '--inference_mode',
        type=str,
        choices=['cloud', 'local'],
        default='cloud',
        help='æ¨ç†æ¨¡å¼: cloud (Google Gemini API) æˆ– local (Unslothæ¨¡å‹) (é»˜è®¤: cloud)'
    )
    parser.add_argument(
        '--model_path',
        type=str,
        default='./deepseek_ocr',
        help='æœ¬åœ°æ¨¡å‹è·¯å¾„ (ä»…åœ¨localæ¨¡å¼ä¸‹ä½¿ç”¨, é»˜è®¤: ./deepseek_ocr)'
    )
    parser.add_argument(
        '--base_model_path',
        type=str,
        default=None,
        help='åŸºç¡€æ¨¡å‹è·¯å¾„ (å½“ model_path æ˜¯ LoRA adapter æ—¶éœ€è¦, é»˜è®¤: ./deepseek_ocr)'
    )
    parser.add_argument(
        '--no-resume',
        action='store_true',
        help='ä¸ä½¿ç”¨æ–­ç‚¹ç»­ä¼ ï¼Œä»å¤´å¼€å§‹å¤„ç†'
    )
    parser.add_argument(
        '--max_new_tokens',
        type=int,
        default=2048,
        help='æœ€å¤§ç”Ÿæˆtokenæ•°ï¼Œé˜²æ­¢ç”Ÿæˆè¿‡é•¿æ–‡æœ¬ (é»˜è®¤: 2048ï¼Œè¡¨æ ¼å¤æ‚å¯å¢å¤§åˆ°4096)'
    )

    args = parser.parse_args()

    # è®¾ç½®é»˜è®¤è¾“å‡ºç›®å½•
    if args.output_dir is None:
        args.output_dir = 'cloud_result' if args.inference_mode == 'cloud' else 'local_result'

    # æ£€æŸ¥æ¨ç†æ¨¡å¼çš„ä¾èµ–
    if args.inference_mode == 'cloud':
        if not CLOUD_API_AVAILABLE:
            print("âœ— é”™è¯¯: Cloud API ä¾èµ–æœªå®‰è£…")
            print("è¯·å®‰è£…: pip install google-genai")
            exit(1)
        if not os.environ.get("GOOGLE_AI_STUDIO_KEY"):
            print("âœ— é”™è¯¯: æœªè®¾ç½®ç¯å¢ƒå˜é‡ GOOGLE_AI_STUDIO_KEY")
            print("è¯·å…ˆè®¾ç½®: export GOOGLE_AI_STUDIO_KEY='your_api_key'")
            exit(1)
    else:  # local mode
        if not UNSLOTH_AVAILABLE:
            print("âœ— é”™è¯¯: Unsloth ä¾èµ–æœªå®‰è£…")
            print("è¯·å®‰è£…: pip install unsloth")
            exit(1)
        if not os.path.exists(args.model_path):
            print(f"âœ— é”™è¯¯: æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {args.model_path}")
            print("è¯·å…ˆä¸‹è½½æ¨¡å‹æˆ–æŒ‡å®šæ­£ç¡®çš„æ¨¡å‹è·¯å¾„")
            exit(1)

    # æ£€æŸ¥split_data_diræ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.split_data_dir):
        print(f"âœ— é”™è¯¯: æ•°æ®ç›®å½•ä¸å­˜åœ¨: {args.split_data_dir}")
        print("è¯·å…ˆè¿è¡Œ split_ocr_data.py è¿›è¡Œæ•°æ®åˆ’åˆ†")
        exit(1)

    resume = not args.no_resume

    # æ ¹æ®data_typeç¡®å®šè¦å¤„ç†çš„ä»»åŠ¡
    if args.data_type == 'all':
        tasks = ['table_ocr', 'stamp_ocr', 'stamp_cls']
    elif args.data_type == 'table':
        tasks = ['table_ocr']
    else:  # stamp
        tasks = ['stamp_ocr', 'stamp_cls']

    print(f"\n{'#' * 80}")
    print(f"æ‰¹é‡æ¨ç†ä»»åŠ¡å¼€å§‹")
    print(f"æ¨ç†æ¨¡å¼: {args.inference_mode.upper()}")
    print(f"æ•°æ®ç±»å‹: {args.data_type}")
    print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
    if args.inference_mode == 'local':
        print(f"æ¨¡å‹è·¯å¾„: {args.model_path}")
    print('#' * 80)

    # æœ¬åœ°æ¨¡å‹ç¼“å­˜ï¼ˆé¿å…é‡å¤åŠ è½½ï¼‰
    local_model_cache = None

    # æ‰¹é‡å¤„ç†æ‰€æœ‰ä»»åŠ¡
    for task_type in tasks:
        try:
            local_model_cache = batch_inference(
                task_type=task_type,
                split_data_dir=args.split_data_dir,
                output_dir=args.output_dir,
                resume=resume,
                inference_mode=args.inference_mode,
                model_path=args.model_path if args.inference_mode == 'local' else None,
                base_model_path=args.base_model_path if args.inference_mode == 'local' else None,
                local_model_cache=local_model_cache,
                max_new_tokens=args.max_new_tokens
            )
        except Exception as e:
            print(f"âœ— ä»»åŠ¡ {task_type} å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            continue

    print(f"\n{'#' * 80}")
    print("âœ“ æ‰€æœ‰æ‰¹é‡æ¨ç†ä»»åŠ¡å®Œæˆï¼")
    print(f"ç»“æœä¿å­˜åœ¨: {args.output_dir}/test/")
    print('#' * 80)


if __name__ == "__main__":
    main()
