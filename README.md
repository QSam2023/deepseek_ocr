# DeepSeek OCR è®­ç»ƒä¸è¯„ä¼°æ¡†æ¶

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)

åŸºäº [Unsloth](https://github.com/unslothai/unsloth) å’Œ [DeepSeek-OCR](https://huggingface.co/unsloth/DeepSeek-OCR) çš„é«˜æ•ˆ OCR æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°æ¡†æ¶ã€‚æ”¯æŒè¡¨æ ¼è¯†åˆ«ã€å°ç« è¯†åˆ«å’Œæ–‡æ¡£ OCR ç­‰å¤šç§ä»»åŠ¡ã€‚

## âœ¨ ä¸»è¦ç‰¹æ€§

- ğŸš€ **ä¸€é”®å¼å®Œæ•´å·¥ä½œæµç¨‹** - ä»æ¨¡å‹æ£€æŸ¥åˆ°ç»“æœå¯¹æ¯”çš„å…¨è‡ªåŠ¨åŒ–æµç¨‹
- ğŸ¯ **å¤šä»»åŠ¡æ”¯æŒ** - æ”¯æŒ Table OCRã€Stamp OCRã€Stamp åˆ†ç±»ä¸‰ç§ä»»åŠ¡
- âš¡ **é«˜æ•ˆè®­ç»ƒ** - åŸºäº Unsloth çš„ LoRA å¾®è°ƒï¼Œæ˜¾å­˜å ç”¨ä½ã€é€Ÿåº¦å¿«
- ğŸ”§ **çµæ´»é…ç½®** - YAML é…ç½®æ–‡ä»¶ + å‘½ä»¤è¡Œå‚æ•°ï¼Œå®Œå…¨å¯å®šåˆ¶
- ğŸ“Š **è‡ªåŠ¨è¯„ä¼°** - è®­ç»ƒå‰åè‡ªåŠ¨å¯¹æ¯”ï¼Œç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
- ğŸ¨ **å‹å¥½ç•Œé¢** - å½©è‰²ç»ˆç«¯è¾“å‡ºï¼Œæ¸…æ™°çš„è¿›åº¦æ˜¾ç¤º
- ğŸ’¾ **æ–­ç‚¹ç»­ä¼ ** - æ”¯æŒä¸­æ–­åç»§ç»­è®­ç»ƒå’Œæ¨ç†
- ğŸ”„ **æ™ºèƒ½åŠ è½½** - è‡ªåŠ¨æ£€æµ‹ LoRA adapter å¹¶åŠ è½½åŸºç¡€æ¨¡å‹

## ğŸ“‹ ç›®å½•

- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [å®‰è£…](#å®‰è£…)
- [ä½¿ç”¨æŒ‡å—](#ä½¿ç”¨æŒ‡å—)
- [é¡¹ç›®ç»“æ„](#é¡¹ç›®ç»“æ„)
- [æ–‡æ¡£](#æ–‡æ¡£)
- [é…ç½®è¯´æ˜](#é…ç½®è¯´æ˜)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)
- [è´¡çŒ®æŒ‡å—](#è´¡çŒ®æŒ‡å—)

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹å¼ 1: ä¸€é”®å®Œæ•´æµç¨‹ï¼ˆæ¨èï¼‰

```bash
# å…‹éš†ä»“åº“
git clone <repository-url>
cd deepseek_ocr

# å®‰è£…ä¾èµ–
pip install -r train_requirements.txt

# è¿è¡Œå®Œæ•´è®­ç»ƒå’Œè¯„ä¼°æµç¨‹ï¼ˆè‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼‰
python train_and_evaluate.py --auto_download_model
```

### æ–¹å¼ 2: å¿«é€Ÿæµ‹è¯•ï¼ˆ10 æ­¥è®­ç»ƒï¼‰

```bash
# å¿«é€ŸéªŒè¯ç¯å¢ƒå’Œæµç¨‹
chmod +x quick_test.sh
./quick_test.sh
```

### æ–¹å¼ 3: åˆ†æ­¥æ‰§è¡Œ

```bash
# 1. æ£€æŸ¥å¹¶ä¸‹è½½æ¨¡å‹
python check_origin_model.py --auto-download

# 2. åˆ’åˆ†æ•°æ®é›†
python split_ocr_data.py --data_type all

# 3. è®­ç»ƒæ¨¡å‹
python train_model.py

# 4. æ¨ç†è¯„ä¼°
python batch_inference.py \
    --inference_mode local \
    --model_path ./lora_model
```

## ğŸ“¦ å®‰è£…

### ç¯å¢ƒè¦æ±‚

- Python 3.8+
- CUDA 11.8+ (GPU è®­ç»ƒæ¨è)
- 16GB+ GPU æ˜¾å­˜ (æ¨è)
- 8GB+ RAM

### ä¾èµ–å®‰è£…

```bash
# åŸºç¡€ä¾èµ–
pip install torch transformers pillow

# Unsloth (åŠ é€Ÿè®­ç»ƒ)
pip install unsloth

# PEFT (LoRA æ”¯æŒ)
pip install peft

# è®­ç»ƒé¢å¤–ä¾èµ–
pip install -r train_requirements.txt
```

### å¯é€‰ä¾èµ–

```bash
# Cloud API æ¨ç†
pip install google-genai

# æ¨¡å‹ä¸‹è½½
pip install huggingface_hub
```

## ğŸ“– ä½¿ç”¨æŒ‡å—

### å®Œæ•´è®­ç»ƒå’Œè¯„ä¼°æµç¨‹

`train_and_evaluate.py` æ•´åˆäº†å®Œæ•´çš„ 6 æ­¥æµç¨‹ï¼š

```bash
# åŸºæœ¬ç”¨æ³•
python train_and_evaluate.py

# è‡ªå®šä¹‰å‚æ•°
python train_and_evaluate.py \
    --data_type stamp \
    --max_steps 100 \
    --learning_rate 2e-4

# è·³è¿‡æŸäº›æ­¥éª¤
python train_and_evaluate.py \
    --skip_model_check \
    --skip_data_split
```

**å·¥ä½œæµç¨‹ï¼š**
1. âœ… æ£€æŸ¥åŸºç¡€æ¨¡å‹ï¼ˆå¯è‡ªåŠ¨ä¸‹è½½ï¼‰
2. âœ… åˆ’åˆ†æ•°æ®é›†ï¼ˆtrain/test splitï¼‰
3. âœ… è®­ç»ƒå‰è¯„ä¼°ï¼ˆåŸºçº¿æ€§èƒ½ï¼‰
4. âœ… è®­ç»ƒæ¨¡å‹ï¼ˆLoRA å¾®è°ƒï¼‰
5. âœ… è®­ç»ƒåè¯„ä¼°ï¼ˆLoRA æ€§èƒ½ï¼‰
6. âœ… å¯¹æ¯”ç»“æœï¼ˆç”ŸæˆæŠ¥å‘Šï¼‰

### å•ç‹¬è®­ç»ƒæ¨¡å‹

ä½¿ç”¨ `train_model.py` åªè¿›è¡Œè®­ç»ƒï¼š

```bash
# ä½¿ç”¨é»˜è®¤é…ç½®
python train_model.py

# ä½¿ç”¨è‡ªå®šä¹‰é…ç½®
python train_model.py --config my_config.yaml

# è¦†ç›–é…ç½®å‚æ•°
python train_model.py \
    --data_type table \
    --max_steps 200 \
    --learning_rate 1e-4
```

### æ‰¹é‡æ¨ç†

ä½¿ç”¨ `batch_inference.py` è¿›è¡Œæ‰¹é‡æ¨ç†ï¼š

```bash
# ä½¿ç”¨åŸºç¡€æ¨¡å‹
python batch_inference.py \
    --inference_mode local \
    --model_path ./deepseek_ocr \
    --data_type all

# ä½¿ç”¨ LoRA æ¨¡å‹ï¼ˆè‡ªåŠ¨æ£€æµ‹å¹¶åŠ è½½åŸºç¡€æ¨¡å‹ï¼‰
python batch_inference.py \
    --inference_mode local \
    --model_path ./lora_model \
    --data_type all

# ä½¿ç”¨ Cloud API
export GOOGLE_AI_STUDIO_KEY='your_api_key'
python batch_inference.py \
    --inference_mode cloud \
    --data_type all
```

### æ•°æ®å‡†å¤‡

#### æ•°æ®æ ¼å¼

æ•°æ®åº”æŒ‰ä»¥ä¸‹ç»“æ„ç»„ç»‡ï¼š

```
ocr_data/
â”œâ”€â”€ stamp_data/
â”‚   â””â”€â”€ stamp_01/
â”‚       â”œâ”€â”€ stamp_0001.png
â”‚       â”œâ”€â”€ stamp_0002.png
â”‚       â”œâ”€â”€ stamp_ocr_01.json          # OCR æ ‡æ³¨
â”‚       â””â”€â”€ stamp_ocr_01_extracted.json # åˆ†ç±»æ ‡æ³¨
â””â”€â”€ table_data/
    â””â”€â”€ table_01/
        â”œâ”€â”€ table_0001.png
        â”œâ”€â”€ table_0002.png
        â””â”€â”€ table_ocr_01.json          # OCR æ ‡æ³¨
```

#### æ ‡æ³¨æ ¼å¼

**OCR æ ‡æ³¨ (stamp_ocr_01.json / table_ocr_01.json):**

```json
{
  "results": [
    {
      "image_path": "stamp_data/stamp_01/stamp_0001.png",
      "prompt": "å¸®æˆ‘æå–å‡ºå›¾ç‰‡ä¸­çš„æ‰€æœ‰ä¿¡æ¯...",
      "result": {
        "å…¬å¸åç§°": "æŸæŸç§‘æŠ€æœ‰é™å…¬å¸",
        "æ—¥æœŸ": "2024-01-01",
        ...
      }
    }
  ]
}
```

**åˆ†ç±»æ ‡æ³¨ (stamp_ocr_01_extracted.json):**

```json
{
  "results": [
    {
      "image_path": "stamp_data/stamp_01/stamp_0001.png",
      "prompt": "å¸®æˆ‘çœ‹ä¸€ä¸‹å›¾ç‰‡ä¸­æ˜¯å¦æœ‰ç›–ç« ...",
      "å…¬ç« ä¿¡æ¯": "æŸæŸç§‘æŠ€æœ‰é™å…¬å¸(*å…¬ç« ä¿¡æ¯*)"
    }
  ]
}
```

#### åˆ’åˆ†æ•°æ®

```bash
# åˆ’åˆ†æ‰€æœ‰æ•°æ®
python split_ocr_data.py --data_type all

# åªåˆ’åˆ† table æ•°æ®
python split_ocr_data.py --data_type table

# è‡ªå®šä¹‰åˆ’åˆ†æ¯”ä¾‹
python split_ocr_data.py \
    --data_type stamp \
    --train_ratio 0.8 \
    --seed 42
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
deepseek_ocr/
â”œâ”€â”€ æ ¸å¿ƒè„šæœ¬
â”‚   â”œâ”€â”€ train_and_evaluate.py        # å®Œæ•´å·¥ä½œæµç¨‹ï¼ˆæ¨èï¼‰
â”‚   â”œâ”€â”€ train_model.py               # è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ batch_inference.py           # æ‰¹é‡æ¨ç†
â”‚   â”œâ”€â”€ check_origin_model.py        # æ¨¡å‹æ£€æŸ¥
â”‚   â”œâ”€â”€ split_ocr_data.py            # æ•°æ®åˆ’åˆ†
â”‚   â””â”€â”€ run_pipeline.py              # åŸå§‹è¯„ä¼°æµç¨‹
â”‚
â”œâ”€â”€ é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ train_config.yaml            # è®­ç»ƒé…ç½®
â”‚   â””â”€â”€ train_requirements.txt       # é¢å¤–ä¾èµ–
â”‚
â”œâ”€â”€ æ•°æ®å¤„ç†
â”‚   â”œâ”€â”€ unsloth_data_collator.py     # æ•°æ®æ•´ç†å™¨
â”‚   â”œâ”€â”€ unsloth_deepseek_ocr.py      # DeepSeek OCR æ¨¡å—
â”‚   â””â”€â”€ eval_utils.py                # è¯„ä¼°å·¥å…·
â”‚
â”œâ”€â”€ è¯„ä¼°è„šæœ¬
â”‚   â”œâ”€â”€ table_ocr_eval/              # Table OCR è¯„ä¼°
â”‚   â”œâ”€â”€ stamp_ocr_eval/              # Stamp OCR è¯„ä¼°
â”‚   â””â”€â”€ stamp_cls_eval/              # Stamp åˆ†ç±»è¯„ä¼°
â”‚
â”œâ”€â”€ æ–‡æ¡£
â”‚   â”œâ”€â”€ README.md                    # æœ¬æ–‡æ¡£
â”‚   â”œâ”€â”€ COMPLETE_WORKFLOW.md         # å®Œæ•´å·¥ä½œæµç¨‹æŒ‡å—
â”‚   â”œâ”€â”€ TRAINING_README.md           # è®­ç»ƒè¯¦ç»†æŒ‡å—
â”‚   â”œâ”€â”€ INFERENCE_WITH_LORA.md       # LoRA æ¨ç†æŒ‡å—
â”‚   â””â”€â”€ README_NEW_FEATURES.md       # æ–°åŠŸèƒ½æ€»è§ˆ
â”‚
â”œâ”€â”€ è¾…åŠ©è„šæœ¬
â”‚   â”œâ”€â”€ quick_test.sh                # å¿«é€Ÿæµ‹è¯•
â”‚   â”œâ”€â”€ deploy_env.sh                # ç¯å¢ƒéƒ¨ç½²
â”‚   â””â”€â”€ test_eval.sh                 # è¯„ä¼°æµ‹è¯•
â”‚
â””â”€â”€ æ•°æ®å’Œæ¨¡å‹
    â”œâ”€â”€ deepseek_ocr/                # åŸºç¡€æ¨¡å‹
    â”œâ”€â”€ ocr_data/                    # è®­ç»ƒæ•°æ®
    â”œâ”€â”€ lora_model/                  # LoRA æ¨¡å‹
    â”œâ”€â”€ baseline_result/             # è®­ç»ƒå‰ç»“æœ
    â””â”€â”€ lora_result/                 # è®­ç»ƒåç»“æœ
```

## ğŸ“š æ–‡æ¡£

- **[å®Œæ•´å·¥ä½œæµç¨‹æŒ‡å—](COMPLETE_WORKFLOW.md)** - train_and_evaluate.py è¯¦ç»†ä½¿ç”¨è¯´æ˜
- **[è®­ç»ƒæŒ‡å—](TRAINING_README.md)** - train_model.py å’Œ train_config.yaml è¯¦ç»†è¯´æ˜
- **[LoRA æ¨ç†æŒ‡å—](INFERENCE_WITH_LORA.md)** - LoRA æ¨¡å‹åŠ è½½å’Œä½¿ç”¨æ–¹æ³•
- **[æ–°åŠŸèƒ½æ€»è§ˆ](README_NEW_FEATURES.md)** - æ‰€æœ‰æ–°å¢åŠŸèƒ½å’Œæ”¹è¿›

## âš™ï¸ é…ç½®è¯´æ˜

### è®­ç»ƒé…ç½® (train_config.yaml)

```yaml
# æ•°æ®é…ç½®
data:
  use_existing_split: false          # æ˜¯å¦ä½¿ç”¨å·²æœ‰æ•°æ®åˆ’åˆ†
  data_type: "all"                   # æ•°æ®ç±»å‹: all, table, stamp
  train_ratio: 0.8                   # è®­ç»ƒé›†æ¯”ä¾‹

# æ¨¡å‹é…ç½®
model:
  model_path: "./deepseek_ocr"       # åŸºç¡€æ¨¡å‹è·¯å¾„
  load_in_4bit: false                # æ˜¯å¦ä½¿ç”¨ 4bit é‡åŒ–
  lora:
    r: 16                            # LoRA rank
    lora_alpha: 16                   # LoRA alpha
    lora_dropout: 0                  # LoRA dropout

# è®­ç»ƒé…ç½®
training:
  per_device_train_batch_size: 2     # Batch size
  gradient_accumulation_steps: 4     # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
  learning_rate: 2e-4                # å­¦ä¹ ç‡
  max_steps: 60                      # æœ€å¤§è®­ç»ƒæ­¥æ•°
  # num_train_epochs: 1              # æˆ–æŒ‡å®šè®­ç»ƒè½®æ•°

# æ•°æ®å¤„ç†é…ç½®
data_processing:
  image_size: 640                    # å›¾åƒå°ºå¯¸
  base_size: 1024                    # åŸºç¡€å°ºå¯¸
  crop_mode: true                    # æ˜¯å¦è£å‰ª
  train_on_responses_only: true      # åªè®­ç»ƒå›å¤

# ä¿å­˜é…ç½®
saving:
  lora_model_path: "lora_model"      # LoRA æ¨¡å‹ä¿å­˜è·¯å¾„
  save_merged_model: false           # æ˜¯å¦ä¿å­˜åˆå¹¶æ¨¡å‹
```

### å‘½ä»¤è¡Œå‚æ•°è¦†ç›–

```bash
# è¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„å‚æ•°
python train_model.py \
    --data_type stamp \
    --max_steps 100 \
    --learning_rate 1e-4 \
    --output_dir my_outputs
```

## ğŸ’¡ ä½¿ç”¨åœºæ™¯ç¤ºä¾‹

### åœºæ™¯ 1: å¿«é€ŸéªŒè¯ç¯å¢ƒ

```bash
# 10 æ­¥å¿«é€Ÿæµ‹è¯•
./quick_test.sh
```

### åœºæ™¯ 2: å®Œæ•´è®­ç»ƒå®éªŒ

```bash
# è®­ç»ƒ 3 ä¸ª epoch
python train_and_evaluate.py \
    --data_type all \
    --num_train_epochs 3 \
    --summary_file experiments/exp_001.json
```

### åœºæ™¯ 3: è¶…å‚æ•°æœç´¢

```bash
# å®éªŒ 1: lr=1e-4
python train_and_evaluate.py \
    --learning_rate 1e-4 \
    --lora_model_path lora_lr1e4 \
    --summary_file exp_lr1e4.json

# å®éªŒ 2: lr=5e-4
python train_and_evaluate.py \
    --learning_rate 5e-4 \
    --lora_model_path lora_lr5e4 \
    --summary_file exp_lr5e4.json
```

### åœºæ™¯ 4: ç‰¹å®šä»»åŠ¡è®­ç»ƒ

```bash
# åªè®­ç»ƒ stamp ä»»åŠ¡
python train_model.py \
    --data_type stamp \
    --max_steps 200
```

### åœºæ™¯ 5: ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹

```bash
# ä½¿ç”¨ LoRA æ¨¡å‹è¿›è¡Œæ¨ç†
python batch_inference.py \
    --inference_mode local \
    --model_path ./lora_model \
    --data_type all
```

## ğŸ¯ æ€§èƒ½ä¼˜åŒ–

### å‡å°‘æ˜¾å­˜å ç”¨

```yaml
# train_config.yaml
model:
  load_in_4bit: true                 # å¯ç”¨ 4bit é‡åŒ–

training:
  per_device_train_batch_size: 1     # å‡å° batch size
  gradient_accumulation_steps: 8     # å¢åŠ ç´¯ç§¯æ­¥æ•°

data_processing:
  image_size: 512                    # å‡å°å›¾åƒå°ºå¯¸
  base_size: 640
  crop_mode: false                   # ç¦ç”¨è£å‰ª
```

### åŠ é€Ÿè®­ç»ƒ

```yaml
model:
  use_gradient_checkpointing: "unsloth"  # ä½¿ç”¨ unsloth åŠ é€Ÿ
  unsloth_force_compile: true            # å¼ºåˆ¶ç¼–è¯‘ä¼˜åŒ–

training:
  dataloader_num_workers: 4              # å¢åŠ æ•°æ®åŠ è½½çº¿ç¨‹
```

## â“ å¸¸è§é—®é¢˜

### Q1: å¦‚ä½•ä¸‹è½½æ¨¡å‹ï¼Ÿ

**A:** è„šæœ¬ä¼šè‡ªåŠ¨æç¤ºä¸‹è½½ï¼Œæˆ–ä½¿ç”¨ï¼š

```bash
python check_origin_model.py --auto-download
```

### Q2: æ˜¾å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ

**A:**
1. å¯ç”¨ 4bit é‡åŒ–ï¼š`load_in_4bit: true`
2. å‡å° batch sizeï¼š`per_device_train_batch_size: 1`
3. å‡å°å›¾åƒå°ºå¯¸ï¼š`image_size: 512, base_size: 640`
4. ç¦ç”¨è£å‰ªï¼š`crop_mode: false`

### Q3: å¦‚ä½•ä½¿ç”¨è®­ç»ƒå¥½çš„ LoRA æ¨¡å‹ï¼Ÿ

**A:**

```bash
python batch_inference.py \
    --inference_mode local \
    --model_path ./lora_model
```

è„šæœ¬ä¼šè‡ªåŠ¨æ£€æµ‹å¹¶åŠ è½½åŸºç¡€æ¨¡å‹ã€‚

### Q4: è®­ç»ƒä¸­æ–­äº†æ€ä¹ˆåŠï¼Ÿ

**A:** ä½¿ç”¨ `--skip_*` å‚æ•°è·³è¿‡å·²å®Œæˆæ­¥éª¤ï¼š

```bash
python train_and_evaluate.py \
    --skip_model_check \
    --skip_data_split \
    --skip_baseline_inference
```

### Q5: å¦‚ä½•å¯¹æ¯”å¤šä¸ªå®éªŒï¼Ÿ

**A:** ä½¿ç”¨ä¸åŒçš„è¾“å‡ºç›®å½•å’Œæ€»ç»“æ–‡ä»¶ï¼š

```bash
python train_and_evaluate.py \
    --lora_model_path exp1/lora \
    --lora_output_dir exp1/result \
    --summary_file exp1.json
```

### Q6: æ”¯æŒå“ªäº›æ•°æ®ç±»å‹ï¼Ÿ

**A:** æ”¯æŒä¸‰ç§æ•°æ®ç±»å‹ï¼š
- `all`: æ‰€æœ‰ä»»åŠ¡ï¼ˆtable_ocr + stamp_ocr + stamp_clsï¼‰
- `table`: è¡¨æ ¼ OCR
- `stamp`: å°ç« ç›¸å…³ï¼ˆstamp_ocr + stamp_clsï¼‰

### Q7: å¦‚ä½•è‡ªå®šä¹‰è®­ç»ƒå‚æ•°ï¼Ÿ

**A:** ä¸¤ç§æ–¹å¼ï¼š
1. ä¿®æ”¹ `train_config.yaml`
2. ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°è¦†ç›–ï¼š
   ```bash
   python train_model.py --max_steps 100 --learning_rate 1e-4
   ```

## ğŸ“Š è¯„ä¼°æŒ‡æ ‡

æ¡†æ¶æ”¯æŒå¤šç§è¯„ä¼°æŒ‡æ ‡ï¼š

- **Table OCR**: å­—ç¬¦çº§å‡†ç¡®ç‡ã€ç¼–è¾‘è·ç¦»
- **Stamp OCR**: å­—ç¬¦çº§å‡†ç¡®ç‡ã€ç¼–è¾‘è·ç¦»
- **Stamp Classification**: å‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡

è¯„ä¼°è„šæœ¬ä¼šè‡ªåŠ¨è®¡ç®—å¹¶è¾“å‡ºè¯¦ç»†æŒ‡æ ‡ã€‚

## ğŸ”§ æ•…éšœæ’é™¤

### æ¨¡å‹åŠ è½½å¤±è´¥

```
TypeError: Unsloth: Cannot determine model type for config file: None
```

**è§£å†³æ–¹æ¡ˆ**: è¿™é€šå¸¸æ˜¯ LoRA adapter ç¼ºå°‘åŸºç¡€æ¨¡å‹ã€‚è„šæœ¬ç°åœ¨ä¼šè‡ªåŠ¨æ£€æµ‹å¹¶åŠ è½½ï¼Œæˆ–æ‰‹åŠ¨æŒ‡å®šï¼š

```bash
python batch_inference.py \
    --model_path ./lora_model \
    --base_model_path ./deepseek_ocr
```

### ç±»å‹é”™è¯¯

```
TypeError: '<=' not supported between instances of 'float' and 'str'
```

**è§£å†³æ–¹æ¡ˆ**: å·²ä¿®å¤ï¼Œæ‰€æœ‰é…ç½®å‚æ•°ç°åœ¨éƒ½æœ‰æ˜¾å¼ç±»å‹è½¬æ¢ã€‚

### è·¯å¾„é”™è¯¯

```
expected str, bytes or os.PathLike object, not NoneType
```

**è§£å†³æ–¹æ¡ˆ**: å·²ä¿®å¤ï¼Œ`output_path` å‚æ•°ç°åœ¨ä¼šè‡ªåŠ¨åˆ›å»ºæœ‰æ•ˆè·¯å¾„ã€‚

## ğŸ¤ è´¡çŒ®æŒ‡å—

æ¬¢è¿è´¡çŒ®ä»£ç ã€æŠ¥å‘Šé—®é¢˜æˆ–æå‡ºå»ºè®®ï¼

### å¼€å‘è®¾ç½®

```bash
# å…‹éš†ä»“åº“
git clone <repository-url>
cd deepseek_ocr

# å®‰è£…å¼€å‘ä¾èµ–
pip install -r train_requirements.txt

# è¿è¡Œæµ‹è¯•
./quick_test.sh
```

### æäº¤ä»£ç 

1. Fork ä»“åº“
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯ï¼š`git checkout -b feature/AmazingFeature`
3. æäº¤æ›´æ”¹ï¼š`git commit -m 'Add some AmazingFeature'`
4. æ¨é€åˆ°åˆ†æ”¯ï¼š`git push origin feature/AmazingFeature`
5. æäº¤ Pull Request

## ğŸ“ æ›´æ–°æ—¥å¿—

### v2.0.0 (2025-12-02)

#### æ–°å¢
- âœ¨ å®Œæ•´çš„è®­ç»ƒå’Œè¯„ä¼°å·¥ä½œæµç¨‹è„šæœ¬ (`train_and_evaluate.py`)
- âœ¨ åŸºäº YAML çš„è®­ç»ƒé…ç½®ç³»ç»Ÿ
- âœ¨ LoRA adapter è‡ªåŠ¨æ£€æµ‹å’ŒåŠ è½½
- âœ¨ è®­ç»ƒå‰åè‡ªåŠ¨å¯¹æ¯”å’ŒæŠ¥å‘Šç”Ÿæˆ
- âœ¨ å½©è‰²ç»ˆç«¯è¾“å‡ºå’Œè¿›åº¦æ˜¾ç¤º
- âœ¨ å®éªŒæ€»ç»“ JSON è¾“å‡º

#### æ”¹è¿›
- ğŸ”§ ä¿®å¤ LoRA æ¨¡å‹åŠ è½½é—®é¢˜
- ğŸ”§ ä¿®å¤é…ç½®å‚æ•°ç±»å‹è½¬æ¢é—®é¢˜
- ğŸ”§ ä¿®å¤æ¨ç† output_path é”™è¯¯
- ğŸ“š å®Œå–„æ–‡æ¡£ç³»ç»Ÿ

#### ä¼˜åŒ–
- âš¡ æ¨¡å‹åŠ è½½ç¼“å­˜ï¼Œé¿å…é‡å¤åŠ è½½
- âš¡ æ–­ç‚¹ç»­ä¼ æ”¯æŒ
- âš¡ å¢é‡ä¿å­˜æ¨ç†ç»“æœ

### v1.0.0

- åˆå§‹ç‰ˆæœ¬
- åŸºç¡€çš„è®­ç»ƒå’Œæ¨ç†åŠŸèƒ½
- Cloud API æ”¯æŒ

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶

## ğŸ™ è‡´è°¢

- [Unsloth](https://github.com/unslothai/unsloth) - é«˜æ•ˆçš„ LoRA è®­ç»ƒæ¡†æ¶
- [DeepSeek](https://huggingface.co/deepseek-ai) - å¼ºå¤§çš„åŸºç¡€æ¨¡å‹
- [Hugging Face](https://huggingface.co/) - æ¨¡å‹æ‰˜ç®¡å’Œå·¥å…·

## ğŸ“® è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼è”ç³»ï¼š

- æäº¤ Issue: [GitHub Issues](https://github.com/your-repo/issues)
- é‚®ä»¶: your-email@example.com

## â­ Star History

å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™æˆ‘ä»¬ä¸€ä¸ª Starï¼

---

**å¿«é€Ÿé“¾æ¥:**
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [å®Œæ•´å·¥ä½œæµç¨‹æŒ‡å—](COMPLETE_WORKFLOW.md)
- [è®­ç»ƒè¯¦ç»†æŒ‡å—](TRAINING_README.md)
- [LoRA æ¨ç†æŒ‡å—](INFERENCE_WITH_LORA.md)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)
