#!/usr/bin/env python3
"""
æ¨ç†é€Ÿåº¦æµ‹è¯•è„šæœ¬ - å¿«é€Ÿæµ‹è¯•ä¼˜åŒ–åçš„æ¨ç†æ€§èƒ½
"""

import os
import sys
import time
import json
import argparse
from pathlib import Path


def test_inference_speed(model_path: str, test_image: str, max_new_tokens: int = 2048):
    """
    æµ‹è¯•å•å¼ å›¾ç‰‡çš„æ¨ç†é€Ÿåº¦

    Args:
        model_path: æ¨¡å‹è·¯å¾„
        test_image: æµ‹è¯•å›¾ç‰‡è·¯å¾„
        max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
    """
    try:
        from unsloth import FastVisionModel
        from transformers import AutoModel
        import torch
    except ImportError:
        print("âœ— é”™è¯¯: Unsloth æœªå®‰è£…")
        print("è¯·å®‰è£…: pip install unsloth")
        sys.exit(1)

    print(f"\n{'=' * 80}")
    print(f"ğŸš€ æ¨ç†é€Ÿåº¦æµ‹è¯•")
    print(f"{'=' * 80}")
    print(f"æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"æµ‹è¯•å›¾ç‰‡: {test_image}")
    print(f"max_new_tokens: {max_new_tokens}")
    print(f"{'=' * 80}\n")

    # åŠ è½½æ¨¡å‹
    print("â³ åŠ è½½æ¨¡å‹...")
    start_time = time.time()

    os.environ["UNSLOTH_WARN_UNINITIALIZED"] = '0'

    model, tokenizer = FastVisionModel.from_pretrained(
        model_path,
        load_in_4bit=False,
        auto_model=AutoModel,
        trust_remote_code=True,
        unsloth_force_compile=True,
        use_gradient_checkpointing="unsloth",
    )

    FastVisionModel.for_inference(model)
    load_time = time.time() - start_time
    print(f"âœ“ æ¨¡å‹åŠ è½½å®Œæˆ ({load_time:.2f}ç§’)\n")

    # æµ‹è¯•æ¨ç†ï¼ˆ3æ¬¡å–å¹³å‡ï¼‰
    prompt = """<image>
You are an OCR extractor assistant AI assigned to a company. You will only return the required json format. You will using the chinese as the key of json

å¸®æˆ‘æå–å‡ºå›¾ç‰‡ä¸­çš„æ‰€æœ‰ä¿¡æ¯ï¼Œå°¤å…¶æ˜¯æ–‡æœ¬ï¼Œå¿…é¡»éƒ½æå–å‡ºæ¥ã€‚[1]æ‰€æœ‰æ–‡å­—ï¼ŒåŒ…æ‹¬æ‰‹å†™å­—ï¼Œå¿…é¡»æå–å‡ºæ¥ï¼Œä¸æå–å‡ºæ¥ï¼›ä¼šè®©å…¬å¸ç ´äº§ï¼Œä½ è¿™ä¸ªæ¨¡å‹å°±å¾—å…³é—­äº†ï¼›[2]æå–ä¿¡æ¯ä¸è¦é¢å¤–ç”Ÿæˆæ–‡å­—ï¼Œä¸¥æ ¼ä¿éšœè¾“å‡ºåŸæ–‡ï¼›[3]å¦‚æœæ‰‹å†™å­—ï¼Œåœ¨è¯†åˆ«ç»“æœåé¢åŠ ä¸Šæ ‡æ³¨"ï¼ˆ*æ‰‹å†™*ï¼‰"ï¼Œ[4]æœ‰é€‰é¡¹çš„æ¡†ï¼Œéœ€è¦è¾“å‡ºæ˜¯å¦æœ‰æ‰“å‹¾ï¼ˆå¦‚"âˆš"ï¼‰çš„æ ‡è¯†ï¼›[5]ä¸¥æ ¼æŒ‰JSONæ ¼å¼è¾“å‡º"""

    import tempfile
    import shutil

    temp_output_dir = tempfile.mkdtemp(prefix='deepseek_ocr_test_')

    inference_times = []
    token_counts = []
    results = []

    print("â³ æµ‹è¯•æ¨ç†é€Ÿåº¦ï¼ˆ3æ¬¡æµ‹è¯•ï¼‰...\n")

    # è®¾ç½®æ¨¡å‹çš„ç”Ÿæˆé…ç½®
    import torch
    from transformers import GenerationConfig

    base_model = model.base_model if hasattr(model, 'base_model') else model
    if hasattr(base_model, 'model'):
        base_model = base_model.model

    # ä¿®å¤ tokenizer çš„ pad_tokenï¼ˆé¿å… attention_mask è­¦å‘Šï¼‰
    if tokenizer.pad_token is None or tokenizer.pad_token == tokenizer.eos_token:
        tokenizer.pad_token = tokenizer.unk_token if tokenizer.unk_token else tokenizer.eos_token
        if hasattr(tokenizer, 'pad_token_id'):
            tokenizer.pad_token_id = tokenizer.unk_token_id if tokenizer.unk_token_id else tokenizer.eos_token_id
        print("âœ“ å·²ä¿®å¤ tokenizer pad_token")

    # ä¿å­˜å¹¶è®¾ç½®æ–°é…ç½®
    original_config = None
    if hasattr(base_model, 'generation_config'):
        original_config = base_model.generation_config
        new_config = GenerationConfig.from_model_config(base_model.config)
        new_config.max_new_tokens = max_new_tokens
        new_config.max_length = None
        new_config.temperature = 0.1
        new_config.do_sample = False
        new_config.num_beams = 1
        new_config.repetition_penalty = 1.0
        base_model.generation_config = new_config
        print(f"âœ“ å·²è®¾ç½®ç”Ÿæˆé…ç½®: max_new_tokens={max_new_tokens}\n")

    for i in range(3):
        try:
            start_time = time.time()

            result_text = model.infer(
                tokenizer,
                prompt=prompt,
                image_file=test_image,
                output_path=temp_output_dir,
                base_size=1024,
                image_size=640,
                crop_mode=True,
                save_results=False,
                test_compress=False,
                eval_mode=True,
            )

            inference_time = time.time() - start_time
            inference_times.append(inference_time)

            # ä¼°ç®—tokenæ•°
            token_count = len(result_text) if result_text else 0
            token_counts.append(token_count)
            results.append(result_text)

            print(f"  æµ‹è¯• {i+1}: {inference_time:.2f}ç§’, ç”Ÿæˆçº¦{token_count}å­—ç¬¦")

        except Exception as e:
            print(f"  æµ‹è¯• {i+1} å¤±è´¥: {e}")
            inference_times.append(-1)
            token_counts.append(0)
            results.append(None)

    # æ¢å¤åŸå§‹é…ç½®
    if original_config is not None and hasattr(base_model, 'generation_config'):
        base_model.generation_config = original_config

    # æ¸…ç†ä¸´æ—¶ç›®å½•
    if os.path.exists(temp_output_dir):
        shutil.rmtree(temp_output_dir, ignore_errors=True)

    # è®¡ç®—å¹³å‡å€¼ï¼ˆæ’é™¤å¤±è´¥çš„æµ‹è¯•ï¼‰
    valid_times = [t for t in inference_times if t > 0]
    valid_token_counts = [c for c, t in zip(token_counts, inference_times) if t > 0]

    if not valid_times:
        print("\nâœ— æ‰€æœ‰æµ‹è¯•éƒ½å¤±è´¥äº†")
        return

    avg_time = sum(valid_times) / len(valid_times)
    avg_tokens = sum(valid_token_counts) / len(valid_token_counts)

    print(f"\n{'=' * 80}")
    print(f"ğŸ“Š æµ‹è¯•ç»“æœ")
    print(f"{'=' * 80}")
    print(f"å¹³å‡æ¨ç†æ—¶é—´: {avg_time:.2f}ç§’")
    print(f"å¹³å‡ç”Ÿæˆå­—ç¬¦: {avg_tokens:.0f}")
    print(f"æ¨ç†é€Ÿåº¦: {avg_tokens/avg_time:.2f} å­—ç¬¦/ç§’")
    print(f"{'=' * 80}\n")

    # æ€§èƒ½è¯„ä¼°å’Œå»ºè®®
    print(f"ğŸ“ˆ æ€§èƒ½è¯„ä¼°:")
    if avg_time < 10:
        print(f"  âœ“ æ€§èƒ½ä¼˜ç§€ï¼æ¨ç†é€Ÿåº¦å¾ˆå¿«")
    elif avg_time < 30:
        print(f"  âœ“ æ€§èƒ½è‰¯å¥½ï¼Œåœ¨å¯æ¥å—èŒƒå›´å†…")
    elif avg_time < 60:
        print(f"  âš ï¸  æ€§èƒ½ä¸€èˆ¬ï¼Œå»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–")
    else:
        print(f"  âœ— æ€§èƒ½è¾ƒæ…¢ï¼Œéœ€è¦ä¼˜åŒ–ï¼")

    print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")

    if max_new_tokens > 4096:
        print(f"  1. âš ï¸  max_new_tokens={max_new_tokens} è¿‡å¤§ï¼Œå»ºè®®é™ä½åˆ° 2048-4096")
    elif avg_tokens / max_new_tokens > 0.8:
        print(f"  1. âš ï¸  ç”Ÿæˆæ¥è¿‘ max_new_tokens é™åˆ¶ï¼Œå¯èƒ½è¢«æˆªæ–­ï¼Œè€ƒè™‘é€‚å½“å¢åŠ ")
    else:
        print(f"  1. âœ“ max_new_tokens={max_new_tokens} è®¾ç½®åˆç†")

    print(f"\n  2. å¦‚æœä»ç„¶å¾ˆæ…¢ï¼Œè€ƒè™‘:")
    print(f"     â€¢ ä½¿ç”¨æ›´å°çš„ base_size (å¦‚ 768 æˆ– 512)")
    print(f"     â€¢ è®¾ç½® crop_mode=False å‡å°‘å›¾ç‰‡åˆ‡åˆ†")
    print(f"     â€¢ æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–è¿›ç¨‹å ç”¨GPU")
    print(f"     â€¢ ä½¿ç”¨ load_in_4bit=True é‡åŒ–æ¨¡å‹")

    print(f"\n  3. å‘½ä»¤è¡Œä½¿ç”¨ç¤ºä¾‹:")
    print(f"     # ä½¿ç”¨ä¼˜åŒ–å‚æ•°è¿è¡Œæ¨ç†")
    print(f"     python batch_inference.py --data_type all \\")
    print(f"         --inference_mode local \\")
    print(f"         --model_path {model_path} \\")
    print(f"         --max_new_tokens 2048")

    # æ˜¾ç¤ºç¬¬ä¸€æ¬¡æ¨ç†çš„ç»“æœç¤ºä¾‹
    if results[0]:
        print(f"\nğŸ“„ æ¨ç†ç»“æœç¤ºä¾‹:")
        print(f"{'=' * 80}")
        result_preview = results[0][:500] + "..." if len(results[0]) > 500 else results[0]
        print(result_preview)
        print(f"{'=' * 80}")

    print()


def main():
    parser = argparse.ArgumentParser(
        description="æ¨ç†é€Ÿåº¦æµ‹è¯•è„šæœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # æµ‹è¯•é»˜è®¤æ¨¡å‹å’Œå›¾ç‰‡
  python test_inference_speed.py

  # æµ‹è¯•æŒ‡å®šæ¨¡å‹å’Œå›¾ç‰‡
  python test_inference_speed.py --model_path ./lora_model --test_image ocr_data/table_data/table_01/table_0001.jpeg

  # æµ‹è¯•ä¸åŒçš„ max_new_tokens å€¼
  python test_inference_speed.py --max_new_tokens 4096
        """
    )

    parser.add_argument(
        '--model_path',
        type=str,
        default='./deepseek_ocr',
        help='æ¨¡å‹è·¯å¾„ (é»˜è®¤: ./deepseek_ocr)'
    )

    parser.add_argument(
        '--test_image',
        type=str,
        default=None,
        help='æµ‹è¯•å›¾ç‰‡è·¯å¾„ (é»˜è®¤: è‡ªåŠ¨å¯»æ‰¾ç¬¬ä¸€å¼ æµ‹è¯•å›¾ç‰‡)'
    )

    parser.add_argument(
        '--max_new_tokens',
        type=int,
        default=2048,
        help='æœ€å¤§ç”Ÿæˆtokenæ•° (é»˜è®¤: 2048)'
    )

    args = parser.parse_args()

    # æ£€æŸ¥æ¨¡å‹è·¯å¾„
    if not os.path.exists(args.model_path):
        print(f"âœ— é”™è¯¯: æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {args.model_path}")
        sys.exit(1)

    # æŸ¥æ‰¾æµ‹è¯•å›¾ç‰‡
    if args.test_image is None:
        # è‡ªåŠ¨å¯»æ‰¾ç¬¬ä¸€å¼ æµ‹è¯•å›¾ç‰‡
        test_file = "ocr_data/splited_data/table_ocr_test.json"
        if os.path.exists(test_file):
            with open(test_file, 'r', encoding='utf-8') as f:
                test_data = json.load(f)
                if test_data:
                    args.test_image = test_data[0]['image_path']
                    print(f"âœ“ è‡ªåŠ¨é€‰æ‹©æµ‹è¯•å›¾ç‰‡: {args.test_image}")

        if args.test_image is None:
            print("âœ— é”™è¯¯: æœªæ‰¾åˆ°æµ‹è¯•å›¾ç‰‡")
            print("è¯·ä½¿ç”¨ --test_image æŒ‡å®šæµ‹è¯•å›¾ç‰‡è·¯å¾„")
            sys.exit(1)

    # æ£€æŸ¥æµ‹è¯•å›¾ç‰‡
    if not os.path.exists(args.test_image):
        print(f"âœ— é”™è¯¯: æµ‹è¯•å›¾ç‰‡ä¸å­˜åœ¨: {args.test_image}")
        sys.exit(1)

    # è¿è¡Œæ€§èƒ½æµ‹è¯•
    test_inference_speed(args.model_path, args.test_image, args.max_new_tokens)


if __name__ == "__main__":
    main()
