#!/usr/bin/env python3
"""
æµ‹è¯•æ¨ç†ç¼“å­˜åŠŸèƒ½
å¯¹æ¯”æœ‰ç¼“å­˜ vs æ— ç¼“å­˜çš„æ¨ç†é€Ÿåº¦
"""

import os
import sys
import time
import json
import argparse


def test_inference_with_cache():
    """æµ‹è¯•æ¨ç†æ—¶ä½¿ç”¨ç¼“å­˜"""
    print("\n" + "=" * 80)
    print("ğŸ§ª æ¨ç†ç¼“å­˜åŠŸèƒ½æµ‹è¯•")
    print("=" * 80)

    # æŸ¥æ‰¾æµ‹è¯•æ•°æ®
    test_json = "ocr_data/splited_data/table_ocr_test.json"
    if not os.path.exists(test_json):
        print(f"âœ— æµ‹è¯•æ•°æ®ä¸å­˜åœ¨: {test_json}")
        print("è¯·å…ˆè¿è¡Œ: python split_ocr_data.py --data_type table --preprocess")
        return

    with open(test_json, 'r', encoding='utf-8') as f:
        test_data = json.load(f)

    if not test_data:
        print("âœ— æµ‹è¯•æ•°æ®ä¸ºç©º")
        return

    # æ‰¾åˆ°ç¬¬ä¸€ä¸ªæœ‰é¢„å¤„ç†ç¼“å­˜çš„æ ·æœ¬
    test_sample = None
    for item in test_data:
        if 'preprocessed_path' in item and os.path.exists(item['preprocessed_path']):
            test_sample = item
            break

    if not test_sample:
        print("âœ— æ²¡æœ‰æ‰¾åˆ°é¢„å¤„ç†ç¼“å­˜")
        print("è¯·å…ˆè¿è¡Œ: python split_ocr_data.py --data_type table --preprocess")
        return

    img_path = test_sample['image_path']
    preprocessed_path = test_sample['preprocessed_path']
    task_type = test_sample['task_type']

    print(f"æµ‹è¯•å›¾ç‰‡: {img_path}")
    print(f"ç¼“å­˜è·¯å¾„: {preprocessed_path}")
    print(f"ä»»åŠ¡ç±»å‹: {task_type}")

    # åŠ è½½æ¨¡å‹
    print("\nâ³ åŠ è½½æ¨¡å‹...")
    from batch_inference import load_local_model, call_local_model

    model_path = "./deepseek_ocr"
    if not os.path.exists(model_path):
        print(f"âœ— æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        return

    start = time.time()
    model, tokenizer = load_local_model(model_path, None)
    load_time = time.time() - start
    print(f"âœ“ æ¨¡å‹åŠ è½½å®Œæˆï¼Œè€—æ—¶: {load_time:.2f} ç§’")

    # æµ‹è¯• 1: ä¸ä½¿ç”¨ç¼“å­˜
    print("\n" + "-" * 80)
    print("ğŸ”¥ æµ‹è¯• 1: å®æ—¶å¤„ç†ï¼ˆä¸ä½¿ç”¨ç¼“å­˜ï¼‰")
    print("-" * 80)

    times_no_cache = []
    for i in range(3):
        print(f"\n  è¿è¡Œ {i+1}/3...")
        start = time.time()
        try:
            result_no_cache = call_local_model(
                img_path=img_path,
                task_type=task_type,
                model=model,
                tokenizer=tokenizer,
                max_new_tokens=2048,
                preprocessed_path=None  # ä¸ä½¿ç”¨ç¼“å­˜
            )
            elapsed = time.time() - start
            times_no_cache.append(elapsed)
            print(f"  âœ“ å®Œæˆï¼Œè€—æ—¶: {elapsed:.2f} ç§’")
        except Exception as e:
            print(f"  âœ— å¤±è´¥: {e}")
            return

    avg_no_cache = sum(times_no_cache) / len(times_no_cache)
    print(f"\nğŸ“Š å¹³å‡æ¨ç†æ—¶é—´ï¼ˆæ— ç¼“å­˜ï¼‰: {avg_no_cache:.2f} ç§’")

    # æµ‹è¯• 2: ä½¿ç”¨ç¼“å­˜
    print("\n" + "-" * 80)
    print("ğŸš€ æµ‹è¯• 2: ä½¿ç”¨ç¼“å­˜")
    print("-" * 80)

    times_with_cache = []
    for i in range(3):
        print(f"\n  è¿è¡Œ {i+1}/3...")
        start = time.time()
        try:
            result_with_cache = call_local_model(
                img_path=img_path,
                task_type=task_type,
                model=model,
                tokenizer=tokenizer,
                max_new_tokens=2048,
                preprocessed_path=preprocessed_path  # ä½¿ç”¨ç¼“å­˜
            )
            elapsed = time.time() - start
            times_with_cache.append(elapsed)
            print(f"  âœ“ å®Œæˆï¼Œè€—æ—¶: {elapsed:.2f} ç§’")
        except Exception as e:
            print(f"  âœ— å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return

    avg_with_cache = sum(times_with_cache) / len(times_with_cache)
    print(f"\nğŸ“Š å¹³å‡æ¨ç†æ—¶é—´ï¼ˆæœ‰ç¼“å­˜ï¼‰: {avg_with_cache:.2f} ç§’")

    # æ€§èƒ½å¯¹æ¯”
    print("\n" + "=" * 80)
    print("ğŸ“ˆ æ€§èƒ½å¯¹æ¯”")
    print("=" * 80)
    print(f"å®æ—¶å¤„ç†: {avg_no_cache:.2f} ç§’")
    print(f"ç¼“å­˜åŠ è½½: {avg_with_cache:.2f} ç§’")
    print(f"é€Ÿåº¦æå‡: {avg_no_cache / avg_with_cache:.1f}x")

    speedup = avg_no_cache / avg_with_cache
    if speedup > 1.5:
        print(f"\nâœ… ç¼“å­˜ä¼˜åŒ–æ˜¾è‘—ï¼é€Ÿåº¦æå‡ {speedup:.1f} å€")
    elif speedup > 1.1:
        print(f"\nâœ“ ç¼“å­˜æœ‰æ•ˆï¼Œé€Ÿåº¦æå‡ {speedup:.1f} å€")
    else:
        print(f"\nâš ï¸  ç¼“å­˜æ•ˆæœä¸æ˜æ˜¾ï¼Œé€Ÿåº¦æå‡ä»… {speedup:.1f} å€")

    # ç»“æœéªŒè¯
    print("\n" + "=" * 80)
    print("âœ… ç»“æœéªŒè¯")
    print("=" * 80)

    print(f"æ— ç¼“å­˜ç»“æœé•¿åº¦: {len(str(result_no_cache))}")
    print(f"æœ‰ç¼“å­˜ç»“æœé•¿åº¦: {len(str(result_with_cache))}")

    # æ˜¾ç¤ºéƒ¨åˆ†ç»“æœ
    result_str = str(result_with_cache)
    if len(result_str) > 200:
        print(f"\nç»“æœé¢„è§ˆ:\n{result_str[:200]}...")
    else:
        print(f"\nç»“æœ:\n{result_str}")

    print("\n" + "=" * 80)
    print("âœ… æµ‹è¯•å®Œæˆ")
    print("=" * 80)


def test_batch_inference():
    """æµ‹è¯•æ‰¹é‡æ¨ç†"""
    print("\n" + "=" * 80)
    print("ğŸ§ª æ‰¹é‡æ¨ç†ç¼“å­˜æµ‹è¯•")
    print("=" * 80)

    test_json = "ocr_data/splited_data/table_ocr_test.json"
    if not os.path.exists(test_json):
        print(f"âœ— æµ‹è¯•æ•°æ®ä¸å­˜åœ¨: {test_json}")
        return

    with open(test_json, 'r', encoding='utf-8') as f:
        test_data = json.load(f)

    # ç»Ÿè®¡ç¼“å­˜æƒ…å†µ
    total = len(test_data)
    with_cache = sum(1 for item in test_data if 'preprocessed_path' in item and os.path.exists(item.get('preprocessed_path', '')))

    print(f"æµ‹è¯•é›†å¤§å°: {total}")
    print(f"æœ‰ç¼“å­˜: {with_cache} ({with_cache/total*100:.1f}%)")
    print(f"æ— ç¼“å­˜: {total - with_cache} ({(total-with_cache)/total*100:.1f}%)")

    if with_cache == 0:
        print("\nâš ï¸  æ²¡æœ‰é¢„å¤„ç†ç¼“å­˜")
        print("è¯·è¿è¡Œ: python split_ocr_data.py --data_type table --preprocess")
        return

    print(f"\nğŸ’¡ æ‰¹é‡æ¨ç†æ—¶ï¼Œ{with_cache} å¼ å›¾ç‰‡å°†ä½¿ç”¨ç¼“å­˜åŠ é€Ÿ")
    print(f"   é¢„è®¡èŠ‚çœæ—¶é—´: {with_cache * 25:.0f} ç§’ (~{with_cache * 25 / 60:.1f} åˆ†é’Ÿ)")

    print("\nä½¿ç”¨æ–¹æ³•:")
    print("  python batch_inference.py \\")
    print("      --data_type table \\")
    print("      --inference_mode local \\")
    print("      --model_path ./deepseek_ocr")


def main():
    parser = argparse.ArgumentParser(description="æµ‹è¯•æ¨ç†ç¼“å­˜åŠŸèƒ½")
    parser.add_argument(
        '--batch',
        action='store_true',
        help='æµ‹è¯•æ‰¹é‡æ¨ç†'
    )

    args = parser.parse_args()

    print("=" * 80)
    print("ğŸš€ æ¨ç†ç¼“å­˜åŠŸèƒ½æµ‹è¯•")
    print("=" * 80)

    # ä¸»æµ‹è¯•ï¼šå•å¼ å›¾ç‰‡æ¨ç†å¯¹æ¯”
    test_inference_with_cache()

    # å¯é€‰ï¼šæ‰¹é‡æ¨ç†ç»Ÿè®¡
    if args.batch:
        test_batch_inference()

    print("\n" + "=" * 80)
    print("ğŸ’¡ ä½¿ç”¨è¯´æ˜")
    print("=" * 80)
    print("\n1. ç¡®ä¿æ•°æ®å·²é¢„å¤„ç†:")
    print("   python split_ocr_data.py --data_type all --preprocess")
    print("\n2. æ‰¹é‡æ¨ç†æ—¶è‡ªåŠ¨ä½¿ç”¨ç¼“å­˜:")
    print("   python batch_inference.py \\")
    print("       --data_type all \\")
    print("       --inference_mode local \\")
    print("       --model_path ./deepseek_ocr")
    print("\n3. ä½¿ç”¨å®Œæ•´æµç¨‹:")
    print("   python train_and_evaluate.py --skip_data_split")
    print()


if __name__ == "__main__":
    main()
