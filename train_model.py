"""
DeepSeek OCR è®­ç»ƒè„šæœ¬
æ”¯æŒä»é…ç½®æ–‡ä»¶åŠ è½½å‚æ•°ï¼Œä½¿ç”¨ unsloth è¿›è¡Œé«˜æ•ˆå¾®è°ƒ
"""

import os
import sys
import json
import yaml
import torch
import argparse
import subprocess
from pathlib import Path
from typing import List, Dict, Any
from PIL import Image

from transformers import AutoModel, Trainer, TrainingArguments
from unsloth import FastVisionModel, is_bf16_supported
from data_collator import DeepSeekOCRDataCollator


def load_config(config_path: str) -> Dict[str, Any]:
    """åŠ è½½ YAML é…ç½®æ–‡ä»¶"""
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config


def split_data_if_needed(config: Dict[str, Any]):
    """æ ¹æ®é…ç½®å†³å®šæ˜¯å¦éœ€è¦é‡æ–°åˆ’åˆ†æ•°æ®"""
    if config['data']['use_existing_split']:
        print("\nä½¿ç”¨ç°æœ‰çš„æ•°æ®åˆ’åˆ†")
        split_dir = config['data']['split_data_dir']
        if not os.path.exists(split_dir):
            raise FileNotFoundError(
                f"åˆ’åˆ†æ•°æ®ç›®å½•ä¸å­˜åœ¨: {split_dir}\n"
                f"è¯·å…ˆè¿è¡Œæ•°æ®åˆ’åˆ†æˆ–å°† use_existing_split è®¾ç½®ä¸º false"
            )
        return

    print("\né‡æ–°åˆ’åˆ†æ•°æ®é›†...")
    data_type = config['data']['data_type']
    data_root = config['data']['data_root']
    split_data_dir = config['data']['split_data_dir']
    train_ratio = config['data']['train_ratio']
    seed = config['data']['split_seed']

    # ç¡®å®šè¦åˆ’åˆ†çš„æ•°æ®ç±»å‹
    types_to_split = []
    if data_type == 'all':
        types_to_split = ['table', 'stamp']
    elif data_type == 'table':
        types_to_split = ['table']
    elif data_type == 'stamp':
        types_to_split = ['stamp']

    # è°ƒç”¨ split_ocr_data.py
    for dtype in types_to_split:
        cmd = [
            sys.executable, 'split_ocr_data.py',
            '--data_type', dtype,
            '--data_root', data_root,
            '--output_dir', split_data_dir,
            '--train_ratio', str(train_ratio),
            '--seed', str(seed)
        ]

        print(f"\nåˆ’åˆ† {dtype} æ•°æ®...")
        print(f"å‘½ä»¤: {' '.join(cmd)}")
        result = subprocess.run(cmd, capture_output=False)

        if result.returncode != 0:
            raise RuntimeError(f"æ•°æ®åˆ’åˆ†å¤±è´¥: {dtype}")

    print("\næ•°æ®åˆ’åˆ†å®Œæˆ")


def load_training_data(config: Dict[str, Any]) -> List[Dict]:
    """åŠ è½½è®­ç»ƒæ•°æ®"""
    split_data_dir = config['data']['split_data_dir']
    data_type = config['data']['data_type']

    train_files = []
    if data_type == 'all':
        train_files = [
            'table_ocr_train.json',
            'stamp_ocr_train.json',
            'stamp_cls_train.json'
        ]
    elif data_type == 'table':
        train_files = ['table_ocr_train.json']
    elif data_type == 'stamp':
        train_files = ['stamp_ocr_train.json', 'stamp_cls_train.json']

    # åŠ è½½æ‰€æœ‰è®­ç»ƒæ•°æ®
    all_train_data = []
    for train_file in train_files:
        file_path = os.path.join(split_data_dir, train_file)
        if not os.path.exists(file_path):
            print(f"è­¦å‘Š: è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ {file_path}ï¼Œè·³è¿‡")
            continue

        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        print(f"åŠ è½½ {train_file}: {len(data)} æ¡æ•°æ®")
        all_train_data.extend(data)

    if not all_train_data:
        raise ValueError("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•è®­ç»ƒæ•°æ®ï¼")

    print(f"\næ€»è®¡åŠ è½½ {len(all_train_data)} æ¡è®­ç»ƒæ•°æ®")
    return all_train_data


def convert_to_conversation(sample: Dict) -> Dict:
    """
    å°†æ•°æ®é›†æ ·æœ¬è½¬æ¢ä¸ºå¯¹è¯æ ¼å¼

    Args:
        sample: åŒ…å« image_path, prompt, result çš„å­—å…¸

    Returns:
        åŒ…å« messages çš„å­—å…¸
    """
    # è¯»å–å›¾åƒ
    image_path = sample['image_path']
    if not os.path.exists(image_path):
        raise FileNotFoundError(f"å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {image_path}")

    image = Image.open(image_path).convert('RGB')

    # æ„å»ºå¯¹è¯
    prompt = sample.get('prompt', '')
    result = sample.get('result', {})

    # ç»Ÿä¸€å°† result è½¬æ¢ä¸º JSON æ ¼å¼
    # å¦‚æœæ˜¯ stringï¼ŒåŒ…è£…æˆ {"result": "..."}
    if isinstance(result, dict):
        result_obj = result
    else:
        result_obj = {"result": str(result)}

    # ç”Ÿæˆ JSON å­—ç¬¦ä¸²å¹¶ç”¨ markdown ä»£ç å—åŒ…è£¹
    result_json = json.dumps(result_obj, ensure_ascii=False, indent=2)
    result_text = f"```json\n{result_json}\n```"

    conversation = [
        {
            "role": "<|User|>",
            "content": f"<image>\n{prompt}",
            "images": [image]
        },
        {
            "role": "<|Assistant|>",
            "content": result_text
        },
    ]

    return {"messages": conversation}


def setup_model_and_tokenizer(config: Dict[str, Any]):
    """è®¾ç½®æ¨¡å‹å’Œ tokenizer"""
    model_config = config['model']

    print("\nåŠ è½½æ¨¡å‹å’Œ tokenizer...")
    print(f"æ¨¡å‹è·¯å¾„: {model_config['model_path']}")

    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ["UNSLOTH_WARN_UNINITIALIZED"] = '0'

    # åŠ è½½æ¨¡å‹
    model, tokenizer = FastVisionModel.from_pretrained(
        model_config['model_path'],
        load_in_4bit=model_config['load_in_4bit'],
        auto_model=AutoModel,
        trust_remote_code=True,
        unsloth_force_compile=model_config['unsloth_force_compile'],
        use_gradient_checkpointing=model_config['use_gradient_checkpointing'],
    )

    print("æ¨¡å‹åŠ è½½å®Œæˆ")

    # é…ç½® LoRA
    lora_config = model_config['lora']
    print("\né…ç½® LoRA...")
    print(f"  r: {lora_config['r']}")
    print(f"  lora_alpha: {lora_config['lora_alpha']}")
    print(f"  target_modules: {lora_config['target_modules']}")

    model = FastVisionModel.get_peft_model(
        model,
        target_modules=lora_config['target_modules'],
        r=int(lora_config['r']),
        lora_alpha=int(lora_config['lora_alpha']),
        lora_dropout=float(lora_config['lora_dropout']),
        bias=str(lora_config['bias']),
        random_state=int(lora_config['random_state']),
        use_rslora=bool(lora_config['use_rslora']),
    )

    # å¯ç”¨è®­ç»ƒæ¨¡å¼
    FastVisionModel.for_training(model)
    print("LoRA é…ç½®å®Œæˆ")

    return model, tokenizer


def create_data_collator(tokenizer, model, config: Dict[str, Any]):
    """åˆ›å»ºæ•°æ®æ•´ç†å™¨"""
    data_proc_config = config['data_processing']

    print("\nåˆ›å»º DataCollator...")
    print(f"  image_size: {data_proc_config['image_size']}")
    print(f"  base_size: {data_proc_config['base_size']}")
    print(f"  crop_mode: {data_proc_config['crop_mode']}")
    print(f"  train_on_responses_only: {data_proc_config['train_on_responses_only']}")

    data_collator = DeepSeekOCRDataCollator(
        tokenizer=tokenizer,
        model=model,
        image_size=int(data_proc_config['image_size']),
        base_size=int(data_proc_config['base_size']),
        crop_mode=bool(data_proc_config['crop_mode']),
        train_on_responses_only=bool(data_proc_config['train_on_responses_only']),
    )

    return data_collator


def create_training_args(config: Dict[str, Any]) -> TrainingArguments:
    """åˆ›å»ºè®­ç»ƒå‚æ•°"""
    train_config = config['training']

    print("\né…ç½®è®­ç»ƒå‚æ•°...")
    print(f"  output_dir: {train_config['output_dir']}")
    print(f"  batch_size: {train_config['per_device_train_batch_size']}")
    print(f"  gradient_accumulation_steps: {train_config['gradient_accumulation_steps']}")
    print(f"  learning_rate: {train_config['learning_rate']}")

    # è®¡ç®—æœ‰æ•ˆ batch size
    effective_batch_size = (
        train_config['per_device_train_batch_size'] *
        train_config['gradient_accumulation_steps']
    )
    print(f"  æœ‰æ•ˆ batch size: {effective_batch_size}")

    # æ ¹æ®é…ç½®å†³å®šä½¿ç”¨ max_steps è¿˜æ˜¯ num_train_epochs
    # ç¡®ä¿æ‰€æœ‰æ•°å€¼å‚æ•°éƒ½æ˜¯æ­£ç¡®çš„ç±»å‹
    training_args_dict = {
        "output_dir": str(train_config['output_dir']),
        "per_device_train_batch_size": int(train_config['per_device_train_batch_size']),
        "gradient_accumulation_steps": int(train_config['gradient_accumulation_steps']),
        "warmup_steps": int(train_config['warmup_steps']),
        "learning_rate": float(train_config['learning_rate']),
        "logging_steps": int(train_config['logging_steps']),
        "optim": str(train_config['optim']),
        "weight_decay": float(train_config['weight_decay']),
        "lr_scheduler_type": str(train_config['lr_scheduler_type']),
        "seed": int(train_config['seed']),
        "dataloader_num_workers": int(train_config['dataloader_num_workers']),
        "save_strategy": str(train_config['save_strategy']),
        "save_steps": int(train_config['save_steps']),
        "save_total_limit": int(train_config['save_total_limit']),
        "report_to": str(train_config['report_to']),
        "remove_unused_columns": False,  # è§†è§‰å¾®è°ƒå¿…é¡»è®¾ç½®
    }

    # ğŸš€ ä¼˜åŒ–ï¼šæ··åˆç²¾åº¦è®­ç»ƒé…ç½®
    # ä¼˜å…ˆä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®ï¼Œå¦åˆ™è‡ªåŠ¨æ£€æµ‹
    if 'bf16' in train_config:
        training_args_dict['bf16'] = bool(train_config['bf16'])
        print(f"  bf16 (é…ç½®æ–‡ä»¶): {train_config['bf16']}")
    else:
        training_args_dict['bf16'] = is_bf16_supported()
        print(f"  bf16 (è‡ªåŠ¨æ£€æµ‹): {is_bf16_supported()}")

    if 'fp16' in train_config:
        training_args_dict['fp16'] = bool(train_config['fp16'])
        print(f"  fp16 (é…ç½®æ–‡ä»¶): {train_config['fp16']}")
    else:
        training_args_dict['fp16'] = not is_bf16_supported()
        print(f"  fp16 (è‡ªåŠ¨æ£€æµ‹): {not is_bf16_supported()}")

    # ğŸš€ ä¼˜åŒ–ï¼šTF32 åŠ é€Ÿï¼ˆA100 æ”¯æŒï¼‰
    if 'tf32' in train_config:
        training_args_dict['tf32'] = bool(train_config['tf32'])
        print(f"  tf32: {train_config['tf32']}")

    # ğŸš€ ä¼˜åŒ–ï¼šDataLoader ä¼˜åŒ–å‚æ•°
    if 'dataloader_prefetch_factor' in train_config:
        # prefetch_factor åªåœ¨ num_workers > 0 æ—¶æœ‰æ•ˆ
        if train_config['dataloader_num_workers'] > 0:
            training_args_dict['dataloader_prefetch_factor'] = int(train_config['dataloader_prefetch_factor'])
            print(f"  dataloader_prefetch_factor: {train_config['dataloader_prefetch_factor']}")

    if 'dataloader_pin_memory' in train_config:
        training_args_dict['dataloader_pin_memory'] = bool(train_config['dataloader_pin_memory'])
        print(f"  dataloader_pin_memory: {train_config['dataloader_pin_memory']}")

    if 'dataloader_persistent_workers' in train_config:
        # persistent_workers åªåœ¨ num_workers > 0 æ—¶æœ‰æ•ˆ
        if train_config['dataloader_num_workers'] > 0:
            training_args_dict['dataloader_persistent_workers'] = bool(train_config['dataloader_persistent_workers'])
            print(f"  dataloader_persistent_workers: {train_config['dataloader_persistent_workers']}")

    # ğŸš€ ä¼˜åŒ–ï¼šæ¢¯åº¦è£å‰ª
    if 'max_grad_norm' in train_config:
        training_args_dict['max_grad_norm'] = float(train_config['max_grad_norm'])
        print(f"  max_grad_norm: {train_config['max_grad_norm']}")

    # æ·»åŠ  max_steps æˆ– num_train_epochs
    if 'num_train_epochs' in train_config and train_config['num_train_epochs'] is not None:
        training_args_dict['num_train_epochs'] = int(train_config['num_train_epochs'])
        print(f"  num_train_epochs: {train_config['num_train_epochs']}")
    else:
        training_args_dict['max_steps'] = int(train_config['max_steps'])
        print(f"  max_steps: {train_config['max_steps']}")

    return TrainingArguments(**training_args_dict)


def print_gpu_stats():
    """æ‰“å° GPU å†…å­˜ä½¿ç”¨æƒ…å†µ"""
    if torch.cuda.is_available():
        gpu_stats = torch.cuda.get_device_properties(0)
        start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
        max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
        print(f"\nGPU = {gpu_stats.name}. Max memory = {max_memory} GB.")
        print(f"{start_gpu_memory} GB of memory reserved.")
    else:
        print("\nè­¦å‘Š: æœªæ£€æµ‹åˆ° GPUï¼Œå°†ä½¿ç”¨ CPU è®­ç»ƒï¼ˆé€Ÿåº¦ä¼šå¾ˆæ…¢ï¼‰")


def save_model(model, tokenizer, config: Dict[str, Any]):
    """ä¿å­˜æ¨¡å‹"""
    save_config = config['saving']

    # ä¿å­˜ LoRA æ¨¡å‹
    lora_path = save_config['lora_model_path']
    print(f"\nä¿å­˜ LoRA æ¨¡å‹åˆ°: {lora_path}")
    model.save_pretrained(lora_path)
    tokenizer.save_pretrained(lora_path)
    print("LoRA æ¨¡å‹ä¿å­˜å®Œæˆ")

    # å¦‚æœéœ€è¦ï¼Œä¿å­˜åˆå¹¶åçš„å®Œæ•´æ¨¡å‹
    if save_config['save_merged_model']:
        merged_path = save_config['merged_model_path']
        print(f"\nä¿å­˜åˆå¹¶åçš„å®Œæ•´æ¨¡å‹åˆ°: {merged_path}")
        model.save_pretrained_merged(merged_path, tokenizer, save_method="merged_16bit")
        print("å®Œæ•´æ¨¡å‹ä¿å­˜å®Œæˆ")


def main():
    parser = argparse.ArgumentParser(
        description="DeepSeek OCR è®­ç»ƒè„šæœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # ä½¿ç”¨é»˜è®¤é…ç½®æ–‡ä»¶
  python train_model.py

  # ä½¿ç”¨è‡ªå®šä¹‰é…ç½®æ–‡ä»¶
  python train_model.py --config my_train_config.yaml

  # è¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„æŸäº›å‚æ•°
  python train_model.py --data_type table --max_steps 100

é…ç½®æ–‡ä»¶è¯´æ˜:
  é…ç½®æ–‡ä»¶ä½¿ç”¨ YAML æ ¼å¼ï¼ŒåŒ…å«ä»¥ä¸‹éƒ¨åˆ†:
  - data: æ•°æ®é…ç½®ï¼ˆæ•°æ®è·¯å¾„ã€åˆ’åˆ†å‚æ•°ç­‰ï¼‰
  - model: æ¨¡å‹é…ç½®ï¼ˆæ¨¡å‹è·¯å¾„ã€LoRA å‚æ•°ç­‰ï¼‰
  - data_processing: æ•°æ®å¤„ç†é…ç½®ï¼ˆå›¾åƒå¤§å°ã€è£å‰ªæ¨¡å¼ç­‰ï¼‰
  - training: è®­ç»ƒé…ç½®ï¼ˆbatch sizeã€å­¦ä¹ ç‡ã€ä¼˜åŒ–å™¨ç­‰ï¼‰
  - saving: æ¨¡å‹ä¿å­˜é…ç½®

  è¯¦ç»†é…ç½®è¯´æ˜è¯·å‚è€ƒ train_config.yaml
        """
    )

    parser.add_argument(
        '--config',
        type=str,
        default='train_config.yaml',
        help='é…ç½®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: train_config.yaml)'
    )

    # å…è®¸é€šè¿‡å‘½ä»¤è¡Œè¦†ç›–æŸäº›å…³é”®å‚æ•°
    parser.add_argument('--data_type', type=str, choices=['all', 'table', 'stamp'],
                        help='è¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„ data_type')
    parser.add_argument('--max_steps', type=int,
                        help='è¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„ max_steps')
    parser.add_argument('--num_train_epochs', type=int,
                        help='è¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„ num_train_epochs')
    parser.add_argument('--learning_rate', type=float,
                        help='è¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„ learning_rate')
    parser.add_argument('--output_dir', type=str,
                        help='è¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„ output_dir')

    args = parser.parse_args()

    # åŠ è½½é…ç½®
    print("=" * 80)
    print("DeepSeek OCR è®­ç»ƒæµç¨‹")
    print("=" * 80)
    print(f"\nåŠ è½½é…ç½®æ–‡ä»¶: {args.config}")

    if not os.path.exists(args.config):
        print(f"é”™è¯¯: é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ {args.config}")
        sys.exit(1)

    config = load_config(args.config)

    # è¦†ç›–é…ç½®ï¼ˆå¦‚æœé€šè¿‡å‘½ä»¤è¡ŒæŒ‡å®šï¼‰
    if args.data_type:
        config['data']['data_type'] = args.data_type
    if args.max_steps:
        config['training']['max_steps'] = args.max_steps
    if args.num_train_epochs:
        config['training']['num_train_epochs'] = args.num_train_epochs
    if args.learning_rate:
        config['training']['learning_rate'] = args.learning_rate
    if args.output_dir:
        config['training']['output_dir'] = args.output_dir

    print("\nå½“å‰é…ç½®:")
    print(f"  æ•°æ®ç±»å‹: {config['data']['data_type']}")
    print(f"  æ¨¡å‹è·¯å¾„: {config['model']['model_path']}")
    print(f"  è¾“å‡ºç›®å½•: {config['training']['output_dir']}")
    print(f"  ä½¿ç”¨ç°æœ‰åˆ’åˆ†: {config['data']['use_existing_split']}")

    try:
        # æ­¥éª¤ 1: åˆ’åˆ†æ•°æ®ï¼ˆå¦‚æœéœ€è¦ï¼‰
        print("\n" + "=" * 80)
        print("æ­¥éª¤ 1/6: æ•°æ®å‡†å¤‡")
        print("=" * 80)
        split_data_if_needed(config)

        # æ­¥éª¤ 2: åŠ è½½è®­ç»ƒæ•°æ®
        print("\n" + "=" * 80)
        print("æ­¥éª¤ 2/6: åŠ è½½è®­ç»ƒæ•°æ®")
        print("=" * 80)
        train_data = load_training_data(config)

        # æ­¥éª¤ 3: è½¬æ¢æ•°æ®æ ¼å¼
        print("\n" + "=" * 80)
        print("æ­¥éª¤ 3/6: è½¬æ¢æ•°æ®æ ¼å¼")
        print("=" * 80)
        print("å°†æ•°æ®è½¬æ¢ä¸ºå¯¹è¯æ ¼å¼...")
        converted_dataset = []
        for i, sample in enumerate(train_data):
            if (i + 1) % 100 == 0:
                print(f"  å¤„ç†è¿›åº¦: {i + 1}/{len(train_data)}")
            try:
                converted_sample = convert_to_conversation(sample)
                converted_dataset.append(converted_sample)
            except Exception as e:
                print(f"  è­¦å‘Š: å¤„ç†æ ·æœ¬ {i} æ—¶å‡ºé”™: {e}")
                continue

        print(f"æ•°æ®è½¬æ¢å®Œæˆï¼Œæœ‰æ•ˆæ ·æœ¬: {len(converted_dataset)}")

        if not converted_dataset:
            raise ValueError("æ²¡æœ‰æœ‰æ•ˆçš„è®­ç»ƒæ ·æœ¬ï¼")

        # æ­¥éª¤ 4: è®¾ç½®æ¨¡å‹
        print("\n" + "=" * 80)
        print("æ­¥éª¤ 4/6: è®¾ç½®æ¨¡å‹å’Œ tokenizer")
        print("=" * 80)
        model, tokenizer = setup_model_and_tokenizer(config)

        # æ­¥éª¤ 5: é…ç½®è®­ç»ƒ
        print("\n" + "=" * 80)
        print("æ­¥éª¤ 5/6: é…ç½®è®­ç»ƒ")
        print("=" * 80)

        data_collator = create_data_collator(tokenizer, model, config)
        training_args = create_training_args(config)

        # åˆ›å»º Trainer
        trainer = Trainer(
            model=model,
            tokenizer=tokenizer,
            data_collator=data_collator,
            train_dataset=converted_dataset,
            args=training_args,
        )

        # æ‰“å° GPU ä¿¡æ¯
        print_gpu_stats()

        # æ­¥éª¤ 6: å¼€å§‹è®­ç»ƒ
        print("\n" + "=" * 80)
        print("æ­¥éª¤ 6/6: å¼€å§‹è®­ç»ƒ")
        print("=" * 80)
        print("\nå¼€å§‹è®­ç»ƒ...\n")

        trainer_stats = trainer.train()

        print("\nè®­ç»ƒå®Œæˆ!")
        print(f"è®­ç»ƒç»Ÿè®¡: {trainer_stats}")

        # ä¿å­˜æ¨¡å‹
        print("\n" + "=" * 80)
        print("ä¿å­˜æ¨¡å‹")
        print("=" * 80)
        save_model(model, tokenizer, config)

        # å®Œæˆ
        print("\n" + "=" * 80)
        print("è®­ç»ƒæµç¨‹å…¨éƒ¨å®Œæˆï¼")
        print("=" * 80)
        print(f"\nLoRA æ¨¡å‹ä¿å­˜åœ¨: {config['saving']['lora_model_path']}")
        if config['saving']['save_merged_model']:
            print(f"å®Œæ•´æ¨¡å‹ä¿å­˜åœ¨: {config['saving']['merged_model_path']}")
        print("\n" + "=" * 80 + "\n")

    except Exception as e:
        print(f"\né”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
